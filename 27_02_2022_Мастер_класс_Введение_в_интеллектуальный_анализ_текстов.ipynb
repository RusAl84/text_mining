{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "27 02 2022 Мастер класс - Введение в интеллектуальный анализ текстов",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNnU3hBmWPgF4qVV5cdSflq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RusAl84/text_mining/blob/master/27_02_2022_%D0%9C%D0%B0%D1%81%D1%82%D0%B5%D1%80_%D0%BA%D0%BB%D0%B0%D1%81%D1%81_%D0%92%D0%B2%D0%B5%D0%B4%D0%B5%D0%BD%D0%B8%D0%B5_%D0%B2_%D0%B8%D0%BD%D1%82%D0%B5%D0%BB%D0%BB%D0%B5%D0%BA%D1%82%D1%83%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D0%B9_%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7_%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%BE%D0%B2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klN_zix5Vqir"
      },
      "source": [
        "#TF-IDF\n",
        "\n",
        "У подхода bag-of-words есть существенный недостаток. Если слово встречается 5 раз в конкретном документе, но и в других документах тоже встречается часто, то его наличие в документе не особо-то о чём-то говорит. Если же слово 5 раз встречается в конкретном документе, но в других документах встречается редко, то его наличие (да ещё и многократное) позволяет хорошо отличать этот документ от других. Однако с точки зрения bag-of-words различий не будет: в обеих ячейках будет просто число 5.\n",
        "\n",
        "Отчасти это решается исключением стоп-слов (и слишком часто встречающихся слов), но лишь отчасти. Другой идеей является отмасштабировать получившуюся таблицу с учётом \"редкости\" слова в наборе документов (т.е. с учётом информативности слова).\n",
        "\n",
        "tfidf=tf∗idf\n",
        "\n",
        "idf=log((N+1)/(Nw+1))+1\n",
        "\n",
        "Здесь tf это частота слова в тексте (то же самое, что в bag of words), N - общее число документов, Nw - число документов, содержащих данное слово.\n",
        "\n",
        "То есть для каждого слова считается отношение общего количества документов к количеству документов, содержащих данное слово (для частых слов оно будет ближе к 1, для редких слов оно будет стремиться к числу, равному количеству документов), и на логарифм от этого числа умножается исходное значение bag-of-words (к числителю и знаменателю прибавляется единичка, чтобы не делить на 0, и к логарифму тоже прибавляется единичка, но это уже технические детали). После этого в sklearn ещё проводится L2-нормализация каждой строки.\n",
        "\n",
        "В sklearn есть класс для поддержки TF-IDF: TfidfVectorizer, рассмотрим его."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rl9lfwIGyuZ",
        "outputId": "f1869e3b-90bc-40f8-ddec-e83bd7d7ac0e"
      },
      "source": [
        "texts = \"\\u0422\\u043E \\u0435\\u0441\\u0442\\u044C \\u0434\\u043B\\u044F \\u043A\\u0430\\u0436\\u0434\\u043E\\u0433\\u043E \\u0441\\u043B\\u043E\\u0432\\u0430 \\u0441\\u0447\\u0438\\u0442\\u0430\\u0435\\u0442\\u0441\\u044F \\u043E\\u0442\\u043D\\u043E\\u0448\\u0435\\u043D\\u0438\\u0435 \\u043E\\u0431\\u0449\\u0435\\u0433\\u043E \\u043A\\u043E\\u043B\\u0438\\u0447\\u0435\\u0441\\u0442\\u0432\\u0430 \\u0434\\u043E\\u043A\\u0443\\u043C\\u0435\\u043D\\u0442\\u043E\\u0432 \\u043A \\u043A\\u043E\\u043B\\u0438\\u0447\\u0435\\u0441\\u0442\\u0432\\u0443 \\u0434\\u043E\\u043A\\u0443\\u043C\\u0435\\u043D\\u0442\\u043E\\u0432, \\u0441\\u043E\\u0434\\u0435\\u0440\\u0436\\u0430\\u0449\\u0438\\u0445 \\u0434\\u0430\\u043D\\u043D\\u043E\\u0435 \\u0441\\u043B\\u043E\\u0432\\u043E (\\u0434\\u043B\\u044F \\u0447\\u0430\\u0441\\u0442\\u044B\\u0445 \\u0441\\u043B\\u043E\\u0432 \\u043E\\u043D\\u043E \\u0431\\u0443\\u0434\\u0435\\u0442 \\u0431\\u043B\\u0438\\u0436\\u0435 \\u043A 1, \\u0434\\u043B\\u044F \\u0440\\u0435\\u0434\\u043A\\u0438\\u0445 \\u0441\\u043B\\u043E\\u0432 \\u043E\\u043D\\u043E \\u0431\\u0443\\u0434\\u0435\\u0442 \\u0441\\u0442\\u0440\\u0435\\u043C\\u0438\\u0442\\u044C\\u0441\\u044F \\u043A \\u0447\\u0438\\u0441\\u043B\\u0443, \\u0440\\u0430\\u0432\\u043D\\u043E\\u043C\\u0443 \\u043A\\u043E\\u043B\\u0438\\u0447\\u0435\\u0441\\u0442\\u0432\\u0443 \\u0434\\u043E\\u043A\\u0443\\u043C\\u0435\\u043D\\u0442\\u043E\\u0432), \\u0438 \\u043D\\u0430 \\u043B\\u043E\\u0433\\u0430\\u0440\\u0438\\u0444\\u043C \\u043E\\u0442 \\u044D\\u0442\\u043E\\u0433\\u043E \\u0447\\u0438\\u0441\\u043B\\u0430 \\u0443\\u043C\\u043D\\u043E\\u0436\\u0430\\u0435\\u0442\\u0441\\u044F \\u0438\\u0441\\u0445\\u043E\\u0434\\u043D\\u043E\\u0435 \\u0437\\u043D\\u0430\\u0447\\u0435\\u043D\\u0438\\u0435 bag-of-words (\\u043A \\u0447\\u0438\\u0441\\u043B\\u0438\\u0442\\u0435\\u043B\\u044E \\u0438 \\u0437\\u043D\\u0430\\u043C\\u0435\\u043D\\u0430\\u0442\\u0435\\u043B\\u044E \\u043F\\u0440\\u0438\\u0431\\u0430\\u0432\\u043B\\u044F\\u0435\\u0442\\u0441\\u044F \\u0435\\u0434\\u0438\\u043D\\u0438\\u0447\\u043A\\u0430, \\u0447\\u0442\\u043E\\u0431\\u044B \\u043D\\u0435 \\u0434\\u0435\\u043B\\u0438\\u0442\\u044C \\u043D\\u0430 0, \\u0438 \\u043A \\u043B\\u043E\\u0433\\u0430\\u0440\\u0438\\u0444\\u043C\\u0443 \\u0442\\u043E\\u0436\\u0435 \\u043F\\u0440\\u0438\\u0431\\u0430\\u0432\\u043B\\u044F\\u0435\\u0442\\u0441\\u044F \\u0435\\u0434\\u0438\\u043D\\u0438\\u0447\\u043A\\u0430, \\u043D\\u043E \\u044D\\u0442\\u043E \\u0443\\u0436\\u0435 \\u0442\\u0435\\u0445\\u043D\\u0438\\u0447\\u0435\\u0441\\u043A\\u0438\\u0435 \\u0434\\u0435\\u0442\\u0430\\u043B\\u0438). \\u041F\\u043E\\u0441\\u043B\\u0435 \\u044D\\u0442\\u043E\\u0433\\u043E \\u0432 sklearn \\u0435\\u0449\\u0451 \\u043F\\u0440\\u043E\\u0432\\u043E\\u0434\\u0438\\u0442\\u0441\\u044F L2-\\u043D\\u043E\\u0440\\u043C\\u0430\\u043B\\u0438\\u0437\\u0430\\u0446\\u0438\\u044F \\u043A\\u0430\\u0436\\u0434\\u043E\\u0439 \\u0441\\u0442\\u0440\\u043E\\u043A\\u0438.\" #@param {type:\"string\"}\n",
        "stexts = texts\n",
        "print(texts)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "То есть для каждого слова считается отношение общего количества документов к количеству документов, содержащих данное слово (для частых слов оно будет ближе к 1, для редких слов оно будет стремиться к числу, равному количеству документов), и на логарифм от этого числа умножается исходное значение bag-of-words (к числителю и знаменателю прибавляется единичка, чтобы не делить на 0, и к логарифму тоже прибавляется единичка, но это уже технические детали). После этого в sklearn ещё проводится L2-нормализация каждой строки.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFgHUfhAVu60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f2565b4-3e54-44b6-f62b-ffdeebdc0b5f"
      },
      "source": [
        "# !pip install scikit-learn\n",
        "!pip install pandas==1.2.3\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas==1.2.3 in /usr/local/lib/python3.7/dist-packages (1.2.3)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.3) (1.21.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.3) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.3) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.2.3) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuJXdzPXPXfF"
      },
      "source": [
        "def printTF_IDF(texts):\n",
        "  records_count = 30\n",
        "  tfIdfTransformer = TfidfVectorizer(ngram_range=(1, 4), use_idf=True, max_features=records_count)\n",
        "  countVectorizer = CountVectorizer(ngram_range=(1, 4), max_features=records_count)\n",
        "  wordCount = countVectorizer.fit_transform([texts])\n",
        "  TfIdf = tfIdfTransformer.fit_transform([texts])\n",
        "  names = countVectorizer.get_feature_names()\n",
        "  df=[]\n",
        "  df = pd.DataFrame(list(names), columns=['names'])\n",
        "  df = df.assign(Word_Count=wordCount.T.todense())\n",
        "  df = df.assign(TF_IDF=TfIdf.T.todense())\n",
        "  df = df.sort_values('TF_IDF', ascending=False)\n",
        "  print(df)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuopkLIjPjkG",
        "outputId": "0a00acc8-51ae-44b7-b813-8aceca0316f7"
      },
      "source": [
        "printTF_IDF(texts)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                names  Word_Count    TF_IDF\n",
            "2                          документов           3  0.325396\n",
            "1                                 для           3  0.325396\n",
            "0                               будет           2  0.216930\n",
            "9                        прибавляется           2  0.216930\n",
            "22                     слов оно будет           2  0.216930\n",
            "21                           слов оно           2  0.216930\n",
            "20                               слов           2  0.216930\n",
            "10              прибавляется единичка           2  0.216930\n",
            "29                              этого           2  0.216930\n",
            "8                           оно будет           2  0.216930\n",
            "4                          количеству           2  0.216930\n",
            "6                                  на           2  0.216930\n",
            "3                            единичка           2  0.216930\n",
            "7                                 оно           2  0.216930\n",
            "5               количеству документов           2  0.216930\n",
            "28              слово для частых слов           1  0.108465\n",
            "27                   слово для частых           1  0.108465\n",
            "26                          слово для           1  0.108465\n",
            "25                              слово           1  0.108465\n",
            "24                    слова считается           1  0.108465\n",
            "23                              слова           1  0.108465\n",
            "19              редких слов оно будет           1  0.108465\n",
            "18                    редких слов оно           1  0.108465\n",
            "17                        редких слов           1  0.108465\n",
            "16                             редких           1  0.108465\n",
            "14      равному количеству документов           1  0.108465\n",
            "13                 равному количеству           1  0.108465\n",
            "12                            равному           1  0.108465\n",
            "11  проводится l2 нормализация каждой           1  0.108465\n",
            "15   равному количеству документов на           1  0.108465\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_rQ7bdtxilr"
      },
      "source": [
        "удалим знаки переноса строк и сделаем текст в нижнем регистре"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIEOKhJGxeKr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a6cabfd-7df8-4cd6-aa56-86bb6ed0b940"
      },
      "source": [
        "  str2 = ''\n",
        "  for item in texts.split():\n",
        "    str2 = str2 + ' ' + item\n",
        "  texts = str(texts)\n",
        "  texts = texts.lower()\n",
        "  # print(texts)\n",
        "  texts = texts.replace('\\n', ' ')\n",
        "  print(texts)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " каждого слова считается отношение общего количества документов количеству документов содержащих данное слово частых слов оно ближе редких слов оно стремиться числу равному количеству документов логарифм числа умножается исходное значение bag of words числителю знаменателю прибавляется единичка делить логарифму прибавляется единичка это технические детали sklearn ещё проводится нормализация каждой строки\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0noHUfJNxy5x"
      },
      "source": [
        "удалим цифры"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnKupWBnxx80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "926d7265-31fc-4f81-ed3b-a41add4dfb02"
      },
      "source": [
        "  str2 = ''\n",
        "  for c in texts:\n",
        "      if c not in ('0', \"1\", '2', '3', '4', '5', '6', '7', '8', '9', '«', '»', '–', \"\\\"\"):\n",
        "          str2 = str2 + c\n",
        "  texts = str2\n",
        "  str2 = ''\n",
        "  print(texts)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " каждого слова считается отношение общего количества документов количеству документов содержащих данное слово частых слов оно ближе редких слов оно стремиться числу равному количеству документов логарифм числа умножается исходное значение bag of words числителю знаменателю прибавляется единичка делить логарифму прибавляется единичка это технические детали sklearn ещё проводится нормализация каждой строки\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99rmNX2Mx6OU"
      },
      "source": [
        "удалим знаки пунктуации"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWd0WHq7x5mI"
      },
      "source": [
        "import string\n",
        "pattern = string.punctuation\n",
        "for c in texts:\n",
        "    if c not in pattern:\n",
        "        str2 = str2 + c\n",
        "    else:\n",
        "        str2 = str2 + \" \"\n",
        "texts = str2\n",
        "str2 = ''"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKuMC8gLyI9s"
      },
      "source": [
        "удалим стоп слова"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Prm51-xVQv0W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb93862c-21dd-4d6e-85af-841c0cf3ca91"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2UZ3uiOXReP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "163d6112-d936-4bad-b233-b0b26088d08c"
      },
      "source": [
        "  from nltk.corpus import stopwords\n",
        "  russian_stopwords = stopwords.words(\"russian\")\n",
        "  for word in texts.split():\n",
        "      if word not in (russian_stopwords):\n",
        "          str2 = str2 + \" \" + word\n",
        "  texts = str2\n",
        "\n",
        "  str2 = ''\n",
        "  for word in texts.split():\n",
        "      if len(word) > 1:\n",
        "          str2 = str2 + \" \" + word\n",
        "  texts = str2\n",
        "  print(texts)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " каждого слова считается отношение общего количества документов количеству документов содержащих данное слово частых слов оно ближе редких слов оно стремиться числу равному количеству документов логарифм числа умножается исходное значение bag of words числителю знаменателю прибавляется единичка делить логарифму прибавляется единичка это технические детали sklearn ещё проводится нормализация каждой строки\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxdYblpjyLkO"
      },
      "source": [
        "рассчитаем TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoQJ1ALGq3yf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f42c9f0f-9580-40ea-ba33-a0307a1b194f"
      },
      "source": [
        "printTF_IDF(texts)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                      names  Word_Count  TF_IDF\n",
            "1                                документов           3   0.381\n",
            "9                              прибавляется           2   0.254\n",
            "2                                  единичка           2   0.254\n",
            "3                                количеству           2   0.254\n",
            "4                     количеству документов           2   0.254\n",
            "7                                       оно           2   0.254\n",
            "22                                     слов           2   0.254\n",
            "23                                 слов оно           2   0.254\n",
            "10                    прибавляется единичка           2   0.254\n",
            "24                           слов оно ближе           1   0.127\n",
            "21               редких слов оно стремиться           1   0.127\n",
            "0                                       bag           1   0.127\n",
            "25                    слов оно ближе редких           1   0.127\n",
            "19                              редких слов           1   0.127\n",
            "26                      слов оно стремиться           1   0.127\n",
            "27                слов оно стремиться числу           1   0.127\n",
            "28                                    слова           1   0.127\n",
            "20                          редких слов оно           1   0.127\n",
            "15                       равному количеству           1   0.127\n",
            "18                                   редких           1   0.127\n",
            "17   равному количеству документов логарифм           1   0.127\n",
            "16            равному количеству документов           1   0.127\n",
            "14                                  равному           1   0.127\n",
            "13    проводится нормализация каждой строки           1   0.127\n",
            "12                  проводится нормализация           1   0.127\n",
            "11                               проводится           1   0.127\n",
            "8                                 оно ближе           1   0.127\n",
            "6   общего количества документов количеству           1   0.127\n",
            "5              общего количества документов           1   0.127\n",
            "29                          слова считается           1   0.127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wy3zm9eTvla"
      },
      "source": [
        "#Суммаризации текстов \n",
        "\n",
        "Задача суммаризации текстов (автореферирование) - одна из ключевых, широко обсуждаемых задач NLP. Она состоит в сжатии больших объемов текста до связного краткого содержания, отражающего только основные идеи.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrZmQkMpRBPJ"
      },
      "source": [
        "#Алгоритм TextRank\n",
        "TextRank - это алгоритм, основанный на PageRank, который часто используется для извлечения ключевых слов и суммирования текста. \n",
        "PageRank (PR) - это алгоритм, используемый для расчета веса веб-страниц. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiBF_PrZUbzG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44f0ca0f-0512-49d7-e62d-87a958175ff7"
      },
      "source": [
        "from gensim.summarization import summarize, keywords\n",
        "print(\"Исходный текст:\")\n",
        "print(stexts)\n",
        "print(\"Результат работы TextRank:\")\n",
        "print(summarize(str(stexts)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Исходный текст:\n",
            "То есть для каждого слова считается отношение общего количества документов к количеству документов, содержащих данное слово (для частых слов оно будет ближе к 1, для редких слов оно будет стремиться к числу, равному количеству документов), и на логарифм от этого числа умножается исходное значение bag-of-words (к числителю и знаменателю прибавляется единичка, чтобы не делить на 0, и к логарифму тоже прибавляется единичка, но это уже технические детали). После этого в sklearn ещё проводится L2-нормализация каждой строки.\n",
            "Результат работы TextRank:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2hy7d1VGVm0"
      },
      "source": [
        "#Алгоритм Rake\n",
        "RAKE: Rapid Automatic Keyword Extraction Algorithm\n",
        "Алгоритм RAKE извлекает ключевые слова с помощью основанного на разделителе подхода, чтобы идентифицировать ключевые слова кандидата и баллы их использующий совместные встречаемости слова, которые появляются в ключевых словах кандидата. Ключевые слова могут содержать несколько лексем. Кроме того, алгоритм RAKE также объединяет ключевые слова, когда они кажутся многократно, разделенными тем же разделителем слияния.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdPdJagoH0Kb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "outputId": "fd60651c-048f-416c-e5be-d4945576b7d3"
      },
      "source": [
        "!pip install rake_nltk\n",
        "from rake_nltk import Metric, Rake\n",
        "r = Rake(language=\"russian\")\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "r.extract_keywords_from_text(stexts)\n",
        "mas = r.get_ranked_phrases()\n",
        "set2 = set()\n",
        "for item in mas:\n",
        "    if not \"nan\" in str(item).replace(\" nan \", \" \"):\n",
        "        set2.add(str(item).replace(\" nan \", \" \"))\n",
        "mas = list(set2)\n",
        "print(\"Исходный текст:\")\n",
        "print(stexts)\n",
        "print(\"Результат работы TextRammk:\")\n",
        "print(str(mas))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rake_nltk\n",
            "  Downloading rake_nltk-1.0.6-py3-none-any.whl (9.1 kB)\n",
            "Collecting nltk<4.0.0,>=3.6.2\n",
            "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (4.62.3)\n",
            "Collecting regex>=2021.8.3\n",
            "  Downloading regex-2022.1.18-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (748 kB)\n",
            "\u001b[K     |████████████████████████████████| 748 kB 37.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (1.1.0)\n",
            "Installing collected packages: regex, nltk, rake-nltk\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.7 rake-nltk-1.0.6 regex-2022.1.18\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nltk",
                  "regex"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Исходный текст:\n",
            "То есть для каждого слова считается отношение общего количества документов к количеству документов, содержащих данное слово (для частых слов оно будет ближе к 1, для редких слов оно будет стремиться к числу, равному количеству документов), и на логарифм от этого числа умножается исходное значение bag-of-words (к числителю и знаменателю прибавляется единичка, чтобы не делить на 0, и к логарифму тоже прибавляется единичка, но это уже технические детали). После этого в sklearn ещё проводится L2-нормализация каждой строки.\n",
            "Результат работы TextRammk:\n",
            "['sklearn ещё проводится l2', '1', 'частых слов оно', 'числителю', '0', 'words', 'знаменателю прибавляется единичка', 'числу', 'это', 'каждого слова считается отношение общего количества документов', 'нормализация каждой строки', 'редких слов оно', 'of', 'прибавляется единичка', 'ближе', 'технические детали ).', 'числа умножается исходное значение bag', 'делить', 'логарифм', 'количеству документов', 'стремиться', 'логарифму', 'равному количеству документов ),', 'содержащих данное слово']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49ShKaewJbds"
      },
      "source": [
        "#Тематическое моделирование\n",
        "Тематическое моделирование — это метод извлечения тем из текста. Latent Dirichlet Allocation (LDA) — популярный алгоритм моделирования тем реализованные в том числе в пакете Gensim. Основная задача алгоритмов ТМ, заключается в том что бы полученные темы были хорошего качество, понятными, самозначимыми и разделенными. Достижение этих целей во многом зависит от качества предварительной обработки текста и стратегии поиска оптимального количества тем. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pP8tUkU-Jzua"
      },
      "source": [
        "##Импорт пакетов\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rax0LrYWJqbw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be1cdbc2-5ef5-4e1c-b6d8-ec3fe1d0a405"
      },
      "source": [
        "import nltk; nltk.download('stopwords')\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "# Plotting tools\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "# Enable logging for gensim - optional\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gpz436soKMkB"
      },
      "source": [
        "Подход LDA к тематическому моделированию заключается в том, что каждый документ рассматривается как набор тем в определенной пропорции. И каждая тема как набор ключевых слов, опять же, в определенной пропорции.\n",
        "\n",
        "После того, как вы укажете алгоритму количество тем, все, что он сделает, — это отобразит распределение тем в документах и распределение ключевых слов по темам.\n",
        "\n",
        "Тема — это не что иное, как набор доминирующих ключевых слов. Просто взглянув на ключевые слова, вы сможете определить, о чем эта тема.\n",
        "\n",
        "Ниже приведены ключевые факторы для получения хороших разделительных тем:\n",
        "\n",
        "Качество обработки текста.\n",
        "Разнообразие тем, о которых говорится в тексте.\n",
        "Выбор алгоритма моделирование тем.\n",
        "Количество тем, указанных в алгоритме.\n",
        "Алгоритмы настройки параметров."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzcGR_TJKQ2A"
      },
      "source": [
        "# Подготовим стоп-слова\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqWlQzsmKXGD"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('russian')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5_VjjTTPGnq"
      },
      "source": [
        "##Лемматизация\n",
        "Лемматизация — это не что иное, как преобразование слова в его корневое слово. Например: лемма слова «machines» — это «machine». Аналогично, «walking» -> «walk», «mice» -> «mouse» и так далее."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNV6UsXyLTxd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "508072a0-f724-4813-f44f-212847a19ab3"
      },
      "source": [
        "!pip install pymorphy2\n",
        "import pymorphy2\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "ltexts = \"\"\n",
        "print(texts)\n",
        "for word in texts.split():\n",
        "    if len(str(word)) > 2:\n",
        "        ltexts+=\" \" + morph.parse(word)[0].normal_form\n",
        "print(ltexts)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 8.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n",
            "Collecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pymorphy2-dicts-ru, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n",
            " каждого слова считается отношение общего количества документов количеству документов содержащих данное слово частых слов оно ближе редких слов оно стремиться числу равному количеству документов логарифм числа умножается исходное значение bag of words числителю знаменателю прибавляется единичка делить логарифму прибавляется единичка это технические детали sklearn ещё проводится нормализация каждой строки\n",
            " каждый слово считаться отношение общий количество документ количество документ содержать данный слово частый слово оно близкий редкий слово оно стремиться число равный количество документ логарифм число умножаться исходный значение bag words числитель знаменатель прибавляться единичка делить логарифм прибавляться единичка это технический деталь sklearn ещё проводиться нормализация каждый строка\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBXQ2V6WPqaR"
      },
      "source": [
        "##Создадим словарь и корпус.\n",
        "Двумя основными входными данными для тематической модели LDA являются словарь (id2word) и корпус. Давайте создадим их."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ckt4JD1YrLj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d1db9b7-0cc0-4caa-9327-9f84cdd91729"
      },
      "source": [
        "from nltk.util import ngrams\n",
        "def make_bigrams(ttexts):\n",
        "    texts = ' '.join(ttexts)\n",
        "    token = nltk.word_tokenize(ttexts)\n",
        "    bigrams = list(ngrams(token, 2))\n",
        "    # print(bigrams)\n",
        "    return bigrams\n",
        "def make_trigrams(ttexts):\n",
        "    texts = ' '.join(ttexts)\n",
        "    token = nltk.word_tokenize(ttexts)\n",
        "    trigrams = list(ngrams(token, 2))\n",
        "    return trigrams\n",
        "\n",
        "btexts = make_bigrams(texts) + make_trigrams(texts)\n",
        "id2word = corpora.Dictionary(btexts)\n",
        "corpus = [id2word.doc2bow(text) for text in btexts]\n",
        "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('каждого', 1), ('слова', 1)]]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMFn6b2iRCuk"
      },
      "source": [
        "Или вы можете увидеть удобочитаемую форму самого корпуса."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuyeZ3OTRFJ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bc93e69-b032-4fcd-a960-50da0d3a85ac"
      },
      "source": [
        "# Human readable format of corpus (term-frequency)\n",
        "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('каждого', 1), ('слова', 1)]]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1juBYJaaRLfH"
      },
      "source": [
        "##Построим тематическую модель\n",
        "У нас есть все необходимое для обучения модели LDA. В дополнение к корпусу и словарю необходимо также указать количество тем."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfhLNkHJRN2b"
      },
      "source": [
        "# Build LDA model\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=5, \n",
        "                                           random_state=100,\n",
        "                                           update_every=1,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha='auto',\n",
        "                                           per_word_topics=True)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWjtOlCbRW43"
      },
      "source": [
        "##Просмотр тем в модели LDA\n",
        "Вышеупомянутая модель LDA построена на 20 различных темах, где каждая тема представляет собой комбинацию ключевых слов, и каждое ключевое слово вносит определенный вес в тему.\n",
        "\n",
        "Вы можете увидеть ключевые слова для каждой темы и вес (важность) каждого ключевого слова, используя lda_model.print_topics()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99Xxhtw7RSxU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acb4f5a0-65ed-4077-85b4-9b9634d88038"
      },
      "source": [
        "lda_model.print_topics()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.116*\"слов\" + 0.116*\"оно\" + 0.115*\"логарифм\" + 0.115*\"документов\" + 0.115*\"количества\" + 0.060*\"каждой\" + 0.060*\"нормализация\" + 0.060*\"числа\" + 0.060*\"общего\" + 0.006*\"sklearn\"'),\n",
              " (1,\n",
              "  '0.156*\"единичка\" + 0.118*\"прибавляется\" + 0.080*\"проводится\" + 0.080*\"исходное\" + 0.080*\"ещё\" + 0.042*\"of\" + 0.042*\"значение\" + 0.042*\"делить\" + 0.042*\"знаменателю\" + 0.042*\"words\"'),\n",
              " (2,\n",
              "  '0.104*\"bag\" + 0.104*\"редких\" + 0.104*\"частых\" + 0.104*\"слов\" + 0.055*\"of\" + 0.055*\"значение\" + 0.054*\"прибавляется\" + 0.054*\"строки\" + 0.054*\"каждой\" + 0.054*\"слово\"'),\n",
              " (3,\n",
              "  '0.080*\"равному\" + 0.080*\"числителю\" + 0.080*\"числу\" + 0.080*\"оно\" + 0.080*\"стремиться\" + 0.080*\"детали\" + 0.080*\"технические\" + 0.042*\"логарифму\" + 0.042*\"ближе\" + 0.042*\"sklearn\"'),\n",
              " (4,\n",
              "  '0.156*\"документов\" + 0.118*\"количеству\" + 0.080*\"слова\" + 0.080*\"данное\" + 0.080*\"считается\" + 0.080*\"отношение\" + 0.080*\"содержащих\" + 0.042*\"слово\" + 0.042*\"числа\" + 0.042*\"умножается\"')]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6IickF1RhIN"
      },
      "source": [
        "##Визуализация темы и ключевых слов\n",
        "Теперь, когда модель LDA создана, следующим шагом является изучение созданных тем и связанных с ними ключевых слов. Нет лучшего инструмента, чем интерактивная диаграмма пакета pyLDAvis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3gAdj_AR78T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "467174a8-0693-4267-ce43-633ac9e6566c"
      },
      "source": [
        "!pip install pyLDAvis==2.1.2\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim  # don't skip this"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyLDAvis==2.1.2\n",
            "  Downloading pyLDAvis-2.1.2.tar.gz (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2) (0.37.1)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2) (1.21.5)\n",
            "Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2) (1.2.3)\n",
            "Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2) (1.1.0)\n",
            "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2) (2.11.3)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2) (2.8.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2) (3.6.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2) (0.16.0)\n",
            "Collecting funcy\n",
            "  Downloading funcy-1.17-py2.py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.7.2->pyLDAvis==2.1.2) (2.0.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.17.0->pyLDAvis==2.1.2) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.17.0->pyLDAvis==2.1.2) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.17.0->pyLDAvis==2.1.2) (1.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from numexpr->pyLDAvis==2.1.2) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->numexpr->pyLDAvis==2.1.2) (3.0.7)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->pyLDAvis==2.1.2) (1.11.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->pyLDAvis==2.1.2) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->pyLDAvis==2.1.2) (8.12.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->pyLDAvis==2.1.2) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->pyLDAvis==2.1.2) (21.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->pyLDAvis==2.1.2) (57.4.0)\n",
            "Building wheels for collected packages: pyLDAvis\n",
            "  Building wheel for pyLDAvis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-2.1.2-py2.py3-none-any.whl size=97738 sha256=7763ae9c216d2581a3a1f08b6ea4d28723c95c8227f8142d8d7b2cd751992ddc\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/fb/41/e32e5312da9f440d34c4eff0d2207b46dc9332a7b931ef1e89\n",
            "Successfully built pyLDAvis\n",
            "Installing collected packages: funcy, pyLDAvis\n",
            "Successfully installed funcy-1.17 pyLDAvis-2.1.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/past/types/oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  from collections import Iterable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mdNzUz9RkAn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 862
        },
        "outputId": "8d10845a-5fed-47c2-82d9-45b55b0dce69"
      },
      "source": [
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
        "vis"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el4211398335679694888963386457\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el4211398335679694888963386457_data = {\"mdsDat\": {\"x\": [-0.10696131506787679, -0.1843139665386415, 0.22842510424976237, -0.06204470673771394, 0.1248948840944702], \"y\": [-0.18080223098088627, -0.0015974924568630223, -0.06295561687988231, 0.19202068350743526, 0.05333465681019645], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [22.671934246811965, 22.655564292738113, 22.618924164709057, 16.900550804754598, 15.153026490986269]}, \"tinfo\": {\"Term\": [\"\\u0434\\u043e\\u043a\\u0443\\u043c\\u0435\\u043d\\u0442\\u043e\\u0432\", \"\\u0435\\u0434\\u0438\\u043d\\u0438\\u0447\\u043a\\u0430\", \"\\u0441\\u043b\\u043e\\u0432\", \"\\u043e\\u043d\\u043e\", \"\\u043b\\u043e\\u0433\\u0430\\u0440\\u0438\\u0444\\u043c\", \"\\u043a\\u043e\\u043b\\u0438\\u0447\\u0435\\u0441\\u0442\\u0432\\u0430\", \"\\u043f\\u0440\\u0438\\u0431\\u0430\\u0432\\u043b\\u044f\\u0435\\u0442\\u0441\\u044f\", \"bag\", \"\\u0447\\u0430\\u0441\\u0442\\u044b\\u0445\", \"\\u0440\\u0435\\u0434\\u043a\\u0438\\u0445\", \"\\u043a\\u043e\\u043b\\u0438\\u0447\\u0435\\u0441\\u0442\\u0432\\u0443\", \"\\u0441\\u043b\\u043e\\u0432\\u0430\", \"\\u0434\\u0430\\u043d\\u043d\\u043e\\u0435\", \"\\u0441\\u0447\\u0438\\u0442\\u0430\\u0435\\u0442\\u0441\\u044f\", \"\\u0440\\u0430\\u0432\\u043d\\u043e\\u043c\\u0443\", \"\\u0447\\u0438\\u0441\\u043b\\u0438\\u0442\\u0435\\u043b\\u044e\", \"\\u0447\\u0438\\u0441\\u043b\\u0443\", \"\\u0441\\u0442\\u0440\\u0435\\u043c\\u0438\\u0442\\u044c\\u0441\\u044f\", \"\\u043e\\u0442\\u043d\\u043e\\u0448\\u0435\\u043d\\u0438\\u0435\", \"\\u0434\\u0435\\u0442\\u0430\\u043b\\u0438\", \"\\u0438\\u0441\\u0445\\u043e\\u0434\\u043d\\u043e\\u0435\", \"\\u0442\\u0435\\u0445\\u043d\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0435\", \"\\u0435\\u0449\\u0451\", \"\\u043f\\u0440\\u043e\\u0432\\u043e\\u0434\\u0438\\u0442\\u0441\\u044f\", \"\\u0441\\u043e\\u0434\\u0435\\u0440\\u0436\\u0430\\u0449\\u0438\\u0445\", \"\\u043a\\u0430\\u0436\\u0434\\u043e\\u0439\", \"\\u0447\\u0438\\u0441\\u043b\\u0430\", \"\\u043d\\u043e\\u0440\\u043c\\u0430\\u043b\\u0438\\u0437\\u0430\\u0446\\u0438\\u044f\", \"\\u043e\\u0431\\u0449\\u0435\\u0433\\u043e\", \"of\", \"\\u0440\\u0430\\u0432\\u043d\\u043e\\u043c\\u0443\", \"\\u0447\\u0438\\u0441\\u043b\\u0438\\u0442\\u0435\\u043b\\u044e\", \"\\u0447\\u0438\\u0441\\u043b\\u0443\", \"\\u0441\\u0442\\u0440\\u0435\\u043c\\u0438\\u0442\\u044c\\u0441\\u044f\", \"\\u0434\\u0435\\u0442\\u0430\\u043b\\u0438\", \"\\u0442\\u0435\\u0445\\u043d\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0435\", \"\\u043e\\u043d\\u043e\", \"\\u043b\\u043e\\u0433\\u0430\\u0440\\u0438\\u0444\\u043c\\u0443\", \"\\u0431\\u043b\\u0438\\u0436\\u0435\", \"sklearn\", \"\\u044d\\u0442\\u043e\", \"\\u0437\\u043d\\u0430\\u043c\\u0435\\u043d\\u0430\\u0442\\u0435\\u043b\\u044e\", \"words\", \"\\u0434\\u0435\\u043b\\u0438\\u0442\\u044c\", \"\\u043a\\u043e\\u043b\\u0438\\u0447\\u0435\\u0441\\u0442\\u0432\\u0443\", \"\\u0441\\u0442\\u0440\\u043e\\u043a\\u0438\", \"\\u043a\\u0430\\u0436\\u0434\\u043e\\u0433\\u043e\", \"\\u0447\\u0430\\u0441\\u0442\\u044b\\u0445\", \"\\u0440\\u0435\\u0434\\u043a\\u0438\\u0445\", \"\\u0443\\u043c\\u043d\\u043e\\u0436\\u0430\\u0435\\u0442\\u0441\\u044f\", \"\\u0438\\u0441\\u0445\\u043e\\u0434\\u043d\\u043e\\u0435\", \"\\u043b\\u043e\\u0433\\u0430\\u0440\\u0438\\u0444\\u043c\", \"\\u043a\\u043e\\u043b\\u0438\\u0447\\u0435\\u0441\\u0442\\u0432\\u0430\", \"\\u043a\\u0430\\u0436\\u0434\\u043e\\u0439\", \"bag\", \"\\u0447\\u0438\\u0441\\u043b\\u0430\", \"\\u043e\\u0431\\u0449\\u0435\\u0433\\u043e\", \"\\u043d\\u043e\\u0440\\u043c\\u0430\\u043b\\u0438\\u0437\\u0430\\u0446\\u0438\\u044f\", \"\\u0441\\u043b\\u043e\\u0432\\u043e\", \"\\u0437\\u043d\\u0430\\u0447\\u0435\\u043d\\u0438\\u0435\", \"\\u0441\\u043b\\u043e\\u0432\", \"\\u043f\\u0440\\u0438\\u0431\\u0430\\u0432\\u043b\\u044f\\u0435\\u0442\\u0441\\u044f\", \"\\u0434\\u043e\\u043a\\u0443\\u043c\\u0435\\u043d\\u0442\\u043e\\u0432\", \"\\u0435\\u0434\\u0438\\u043d\\u0438\\u0447\\u043a\\u0430\", \"\\u043f\\u0440\\u043e\\u0432\\u043e\\u0434\\u0438\\u0442\\u0441\\u044f\", \"\\u0441\\u0447\\u0438\\u0442\\u0430\\u0435\\u0442\\u0441\\u044f\", \"\\u0435\\u0434\\u0438\\u043d\\u0438\\u0447\\u043a\\u0430\", \"\\u0435\\u0449\\u0451\", \"\\u043f\\u0440\\u043e\\u0432\\u043e\\u0434\\u0438\\u0442\\u0441\\u044f\", \"\\u0438\\u0441\\u0445\\u043e\\u0434\\u043d\\u043e\\u0435\", \"\\u043f\\u0440\\u0438\\u0431\\u0430\\u0432\\u043b\\u044f\\u0435\\u0442\\u0441\\u044f\", \"of\", \"\\u0437\\u043d\\u0430\\u0447\\u0435\\u043d\\u0438\\u0435\", \"\\u043d\\u043e\\u0440\\u043c\\u0430\\u043b\\u0438\\u0437\\u0430\\u0446\\u0438\\u044f\", \"\\u0434\\u0435\\u043b\\u0438\\u0442\\u044c\", \"\\u0437\\u043d\\u0430\\u043c\\u0435\\u043d\\u0430\\u0442\\u0435\\u043b\\u044e\", \"words\", \"\\u044d\\u0442\\u043e\", \"\\u0443\\u043c\\u043d\\u043e\\u0436\\u0430\\u0435\\u0442\\u0441\\u044f\", \"sklearn\", \"\\u0441\\u0442\\u0440\\u043e\\u043a\\u0438\", \"\\u043a\\u0430\\u0436\\u0434\\u043e\\u0433\\u043e\", \"\\u043a\\u043e\\u043b\\u0438\\u0447\\u0435\\u0441\\u0442\\u0432\\u0430\", \"\\u043e\\u0431\\u0449\\u0435\\u0433\\u043e\", \"\\u0441\\u043e\\u0434\\u0435\\u0440\\u0436\\u0430\\u0449\\u0438\\u0445\", \"\\u043e\\u0442\\u043d\\u043e\\u0448\\u0435\\u043d\\u0438\\u0435\", \"\\u043b\\u043e\\u0433\\u0430\\u0440\\u0438\\u0444\\u043c\", \"\\u043a\\u0430\\u0436\\u0434\\u043e\\u0439\", \"\\u0447\\u0430\\u0441\\u0442\\u044b\\u0445\", \"\\u0440\\u0435\\u0434\\u043a\\u0438\\u0445\", \"bag\", \"\\u0447\\u0438\\u0441\\u043b\\u0430\", \"\\u0441\\u043b\\u043e\\u0432\\u043e\", \"\\u0431\\u043b\\u0438\\u0436\\u0435\", \"\\u043b\\u043e\\u0433\\u0430\\u0440\\u0438\\u0444\\u043c\\u0443\", \"\\u0434\\u0430\\u043d\\u043d\\u043e\\u0435\", \"\\u0434\\u043e\\u043a\\u0443\\u043c\\u0435\\u043d\\u0442\\u043e\\u0432\", \"\\u0447\\u0438\\u0441\\u043b\\u0443\", \"\\u043a\\u043e\\u043b\\u0438\\u0447\\u0435\\u0441\\u0442\\u0432\\u0443\", \"\\u043e\\u043d\\u043e\", \"\\u0441\\u043b\\u043e\\u0432\", \"\\u0434\\u0435\\u0442\\u0430\\u043b\\u0438\", \"\\u0441\\u0442\\u0440\\u0435\\u043c\\u0438\\u0442\\u044c\\u0441\\u044f\", \"\\u0441\\u043b\\u043e\\u0432\\u0430\", \"\\u0434\\u0430\\u043d\\u043d\\u043e\\u0435\", \"\\u0441\\u0447\\u0438\\u0442\\u0430\\u0435\\u0442\\u0441\\u044f\", \"\\u043e\\u0442\\u043d\\u043e\\u0448\\u0435\\u043d\\u0438\\u0435\", \"\\u0441\\u043e\\u0434\\u0435\\u0440\\u0436\\u0430\\u0449\\u0438\\u0445\", \"\\u043a\\u0430\\u0436\\u0434\\u043e\\u0433\\u043e\", \"\\u043a\\u043e\\u043b\\u0438\\u0447\\u0435\\u0441\\u0442\\u0432\\u0443\", \"\\u0434\\u043e\\u043a\\u0443\\u043c\\u0435\\u043d\\u0442\\u043e\\u0432\", \"\\u0441\\u043b\\u043e\\u0432\\u043e\", \"\\u0447\\u0438\\u0441\\u043b\\u0430\", \"\\u043e\\u0431\\u0449\\u0435\\u0433\\u043e\", \"\\u0443\\u043c\\u043d\\u043e\\u0436\\u0430\\u0435\\u0442\\u0441\\u044f\", \"\\u0441\\u0442\\u0440\\u043e\\u043a\\u0438\", \"\\u043a\\u0430\\u0436\\u0434\\u043e\\u0439\", \"\\u0447\\u0430\\u0441\\u0442\\u044b\\u0445\", \"\\u0442\\u0435\\u0445\\u043d\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0435\", \"\\u0434\\u0435\\u0442\\u0430\\u043b\\u0438\", \"\\u043b\\u043e\\u0433\\u0430\\u0440\\u0438\\u0444\\u043c\", \"\\u043a\\u043e\\u043b\\u0438\\u0447\\u0435\\u0441\\u0442\\u0432\\u0430\", \"bag\", \"\\u0440\\u0435\\u0434\\u043a\\u0438\\u0445\", \"\\u043d\\u043e\\u0440\\u043c\\u0430\\u043b\\u0438\\u0437\\u0430\\u0446\\u0438\\u044f\", \"\\u0437\\u043d\\u0430\\u0447\\u0435\\u043d\\u0438\\u0435\", \"of\", \"\\u0431\\u043b\\u0438\\u0436\\u0435\", \"\\u043b\\u043e\\u0433\\u0430\\u0440\\u0438\\u0444\\u043c\\u0443\", \"\\u0435\\u0449\\u0451\", \"\\u0438\\u0441\\u0445\\u043e\\u0434\\u043d\\u043e\\u0435\", \"\\u043f\\u0440\\u043e\\u0432\\u043e\\u0434\\u0438\\u0442\\u0441\\u044f\", \"\\u0437\\u043d\\u0430\\u043c\\u0435\\u043d\\u0430\\u0442\\u0435\\u043b\\u044e\", \"\\u044d\\u0442\\u043e\", \"\\u0441\\u043b\\u043e\\u0432\", \"\\u0435\\u0434\\u0438\\u043d\\u0438\\u0447\\u043a\\u0430\", \"\\u043f\\u0440\\u0438\\u0431\\u0430\\u0432\\u043b\\u044f\\u0435\\u0442\\u0441\\u044f\", \"\\u043e\\u043d\\u043e\", \"words\", \"bag\", \"\\u0440\\u0435\\u0434\\u043a\\u0438\\u0445\", \"\\u0447\\u0430\\u0441\\u0442\\u044b\\u0445\", \"\\u0441\\u0442\\u0440\\u043e\\u043a\\u0438\", \"\\u0441\\u043b\\u043e\\u0432\", \"\\u043a\\u0430\\u0436\\u0434\\u043e\\u0439\", \"of\", \"\\u0437\\u043d\\u0430\\u0447\\u0435\\u043d\\u0438\\u0435\", \"\\u0441\\u043b\\u043e\\u0432\\u043e\", \"\\u043b\\u043e\\u0433\\u0430\\u0440\\u0438\\u0444\\u043c\\u0443\", \"\\u0431\\u043b\\u0438\\u0436\\u0435\", \"\\u043f\\u0440\\u0438\\u0431\\u0430\\u0432\\u043b\\u044f\\u0435\\u0442\\u0441\\u044f\", \"\\u043a\\u0430\\u0436\\u0434\\u043e\\u0433\\u043e\", \"\\u043d\\u043e\\u0440\\u043c\\u0430\\u043b\\u0438\\u0437\\u0430\\u0446\\u0438\\u044f\", \"\\u043f\\u0440\\u043e\\u0432\\u043e\\u0434\\u0438\\u0442\\u0441\\u044f\", \"\\u043b\\u043e\\u0433\\u0430\\u0440\\u0438\\u0444\\u043c\", \"\\u043a\\u043e\\u043b\\u0438\\u0447\\u0435\\u0441\\u0442\\u0432\\u0430\", \"\\u043e\\u0431\\u0449\\u0435\\u0433\\u043e\", \"\\u0447\\u0438\\u0441\\u043b\\u0430\", \"\\u0441\\u043e\\u0434\\u0435\\u0440\\u0436\\u0430\\u0449\\u0438\\u0445\", \"\\u0441\\u0447\\u0438\\u0442\\u0430\\u0435\\u0442\\u0441\\u044f\", \"\\u043e\\u0442\\u043d\\u043e\\u0448\\u0435\\u043d\\u0438\\u0435\", \"\\u0434\\u0430\\u043d\\u043d\\u043e\\u0435\", \"\\u0441\\u043b\\u043e\\u0432\\u0430\", \"\\u0443\\u043c\\u043d\\u043e\\u0436\\u0430\\u0435\\u0442\\u0441\\u044f\", \"\\u0435\\u0449\\u0451\", \"\\u0438\\u0441\\u0445\\u043e\\u0434\\u043d\\u043e\\u0435\", \"sklearn\", \"words\", \"\\u0434\\u0435\\u043b\\u0438\\u0442\\u044c\", \"\\u0442\\u0435\\u0445\\u043d\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0435\", \"\\u0447\\u0438\\u0441\\u043b\\u0438\\u0442\\u0435\\u043b\\u044e\", \"\\u0434\\u0435\\u0442\\u0430\\u043b\\u0438\", \"\\u0437\\u043d\\u0430\\u043c\\u0435\\u043d\\u0430\\u0442\\u0435\\u043b\\u044e\", \"\\u0435\\u0434\\u0438\\u043d\\u0438\\u0447\\u043a\\u0430\", \"\\u0441\\u0442\\u0440\\u0435\\u043c\\u0438\\u0442\\u044c\\u0441\\u044f\", \"\\u0434\\u043e\\u043a\\u0443\\u043c\\u0435\\u043d\\u0442\\u043e\\u0432\", \"\\u043a\\u043e\\u043b\\u0438\\u0447\\u0435\\u0441\\u0442\\u0432\\u0443\", \"\\u043e\\u043d\\u043e\", \"\\u043b\\u043e\\u0433\\u0430\\u0440\\u0438\\u0444\\u043c\", \"\\u043a\\u043e\\u043b\\u0438\\u0447\\u0435\\u0441\\u0442\\u0432\\u0430\", \"\\u0441\\u043b\\u043e\\u0432\", \"\\u043e\\u043d\\u043e\", \"\\u043a\\u0430\\u0436\\u0434\\u043e\\u0439\", \"\\u0447\\u0438\\u0441\\u043b\\u0430\", \"\\u043d\\u043e\\u0440\\u043c\\u0430\\u043b\\u0438\\u0437\\u0430\\u0446\\u0438\\u044f\", \"\\u043e\\u0431\\u0449\\u0435\\u0433\\u043e\", \"\\u0434\\u043e\\u043a\\u0443\\u043c\\u0435\\u043d\\u0442\\u043e\\u0432\", \"\\u0441\\u0442\\u0440\\u043e\\u043a\\u0438\", \"\\u043a\\u0430\\u0436\\u0434\\u043e\\u0433\\u043e\", \"\\u0440\\u0435\\u0434\\u043a\\u0438\\u0445\", \"\\u0435\\u0449\\u0451\", \"sklearn\", \"bag\", \"\\u0447\\u0430\\u0441\\u0442\\u044b\\u0445\", \"\\u0441\\u0442\\u0440\\u0435\\u043c\\u0438\\u0442\\u044c\\u0441\\u044f\", \"\\u0441\\u043b\\u043e\\u0432\\u043e\", \"\\u0437\\u043d\\u0430\\u0447\\u0435\\u043d\\u0438\\u0435\", \"of\", \"\\u0431\\u043b\\u0438\\u0436\\u0435\", \"\\u043b\\u043e\\u0433\\u0430\\u0440\\u0438\\u0444\\u043c\\u0443\", \"\\u0441\\u043e\\u0434\\u0435\\u0440\\u0436\\u0430\\u0449\\u0438\\u0445\", \"\\u043e\\u0442\\u043d\\u043e\\u0448\\u0435\\u043d\\u0438\\u0435\", \"\\u0441\\u0447\\u0438\\u0442\\u0430\\u0435\\u0442\\u0441\\u044f\", \"\\u0434\\u0430\\u043d\\u043d\\u043e\\u0435\", \"\\u0441\\u043b\\u043e\\u0432\\u0430\", \"\\u0443\\u043c\\u043d\\u043e\\u0436\\u0430\\u0435\\u0442\\u0441\\u044f\", \"\\u043f\\u0440\\u043e\\u0432\\u043e\\u0434\\u0438\\u0442\\u0441\\u044f\", \"\\u0438\\u0441\\u0445\\u043e\\u0434\\u043d\\u043e\\u0435\", \"\\u0434\\u0435\\u043b\\u0438\\u0442\\u044c\", \"\\u0442\\u0435\\u0445\\u043d\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0435\", \"\\u0447\\u0438\\u0441\\u043b\\u0438\\u0442\\u0435\\u043b\\u044e\", \"\\u0447\\u0438\\u0441\\u043b\\u0443\", \"\\u0434\\u0435\\u0442\\u0430\\u043b\\u0438\", \"\\u043a\\u043e\\u043b\\u0438\\u0447\\u0435\\u0441\\u0442\\u0432\\u0443\", \"\\u0435\\u0434\\u0438\\u043d\\u0438\\u0447\\u043a\\u0430\", \"\\u043f\\u0440\\u0438\\u0431\\u0430\\u0432\\u043b\\u044f\\u0435\\u0442\\u0441\\u044f\", \"\\u044d\\u0442\\u043e\"], \"Freq\": [10.0, 7.0, 7.0, 7.0, 4.0, 4.0, 7.0, 4.0, 4.0, 4.0, 7.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 3.4860735307861734, 3.4860031523441526, 3.485977530699362, 3.481995473044388, 3.478663686247716, 3.4786351456813667, 3.4827608791419395, 1.8344410102108357, 1.8344275507392052, 1.8264000299665162, 1.826133272968535, 1.8261141378160959, 1.8261105702453024, 1.8260906242813195, 1.821266944243657, 0.1660530488081105, 0.16605032245429377, 0.17337479867231032, 0.17336117703837084, 0.17347586633127193, 0.17340710951233934, 0.16609431911570094, 0.16610303533979914, 0.16608866370518144, 0.16609232249227382, 0.16610573128818298, 0.16611442724199255, 0.16611016034624218, 0.1661073833167039, 0.16611826846168234, 0.1735332717885883, 0.17355368396920892, 0.16633790717381208, 0.16617889689488616, 0.16611314007866643, 0.16611268399717294, 6.792861155744807, 3.475707637627895, 3.475743287576684, 3.4757099062609997, 5.1384957899548995, 1.8252533953105985, 1.82521596286437, 1.817632732621257, 1.8244198346898202, 1.8244068710720787, 1.8244063849364132, 1.8243888840524622, 1.816971912206884, 1.8168088947137844, 0.16591431476198432, 0.16591227906888584, 0.19059769112269037, 0.19091566435910615, 0.1806634480351978, 0.17326557727013542, 0.16595403609697038, 0.16596535900684148, 0.16597345114010356, 0.16595493747351647, 0.1659367276417202, 0.16595317523172973, 0.16597413983229606, 0.1659637588102765, 0.16594434376714334, 0.16595627434659607, 0.1881370344301316, 0.16597663127758078, 0.16602988338858451, 0.16601417512989933, 0.16600027975213263, 0.16597261053051562, 0.16596857965562412, 3.479831655419304, 3.4797601472664943, 3.4797342619623097, 3.4724316940855404, 3.46502526142573, 1.8227483157508217, 5.141620943388541, 6.783128771962867, 1.8309618845517368, 1.8233986840184588, 1.8159739698625685, 1.8228419881953395, 0.17304756595024784, 0.17308590855707123, 0.17307177275423918, 0.1731093468910946, 0.1731014195166881, 0.16578415970750454, 0.16580095482088367, 0.16579669790171894, 0.1658019052968967, 0.1658221281907909, 0.16584788204616516, 0.16581209763541938, 0.16581336156628776, 0.1657965361185678, 0.165813129003008, 0.16581316944879576, 0.16578259243322774, 0.1658229269950997, 0.16581924642841098, 0.1659200272201327, 0.16588952098469328, 0.16588330244482083, 0.16587160350070304, 0.16580667789985573, 3.3798542592198793, 3.3657335308113643, 3.365707178500913, 1.763333303604359, 3.3654970853102513, 1.7632236634870226, 1.769693880995761, 1.7696504843010268, 1.762450259439925, 1.7623301025748397, 1.7622921455772174, 1.7664110842300476, 0.16100515238408092, 0.16813653485618282, 0.1681048486205083, 0.16104412176977692, 0.16105542425155306, 0.16107978200639678, 0.16104233875794594, 0.16105855207739217, 0.16103300061123785, 0.1610304771961889, 0.16102006622032827, 0.16101961291223565, 0.1610767750627157, 0.16103020521133332, 0.16102876973570668, 0.16105137469925893, 0.16105533358993454, 0.16104368357195406, 0.16106857018623918, 0.161047128713458, 0.16104519459892946, 0.16103874251374445, 0.16817104671230138, 0.1610387576240142, 0.16126445972333234, 0.16112286138546594, 0.16107762123782193, 3.3585989166192216, 3.3347995512816744, 3.3659104263952138, 3.361949898819846, 1.7594435116506497, 1.7586896002660988, 1.7588228028382111, 1.741739383294754, 3.347711855294016, 0.15997883610476082, 0.15997080222302135, 0.16704694500498282, 0.16706118380718887, 0.1670985081650173, 0.16004031629925286, 0.16002540010398275, 0.1638871366748911, 0.16003214693890055, 0.1600369835253778, 0.16000773369287266, 0.16003737641335494, 0.16002190475577233, 0.16004538319937184, 0.1600426058878093, 0.16004026210780772, 0.1600266465072206, 0.1600117167640892, 0.16005173714631252, 0.1600113509718346, 0.16001785394524937, 0.16004467871058525, 0.16005183198134149, 0.16004607414029717, 0.16004284974931235, 0.16003883958237325, 0.16014051628128523, 0.160110006497681, 0.16009067369963337, 0.16003061603057583], \"Total\": [10.0, 7.0, 7.0, 7.0, 4.0, 4.0, 7.0, 4.0, 4.0, 4.0, 7.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.138836675052952, 4.138834467386143, 4.138835323826395, 4.138688667190026, 4.138821750476223, 4.138820450010967, 7.337674177830209, 4.088533897427158, 4.088534193106342, 4.13714101574282, 4.137403237079391, 4.137403296105969, 4.137402361692019, 4.137402061261573, 7.450181148687534, 2.4283270692294625, 2.4756868718811034, 4.038152601171549, 4.037898495625131, 4.1344182789425235, 4.135976808903091, 4.017475553309175, 4.018356656816601, 4.027807106406766, 4.037720323554845, 4.075189529562416, 4.075823226764818, 4.0765243588526845, 4.0855258140795625, 4.086869581198622, 7.236861090466318, 7.404434534298611, 10.64658002858416, 7.453210626834369, 4.135755219680921, 4.132857584501136, 7.453210626834369, 4.135709647065291, 4.135755219680921, 4.135976808903091, 7.404434534298611, 4.086869301757474, 4.086869581198622, 4.0765243588526845, 4.137402061261573, 4.137403296105969, 4.137402361692019, 4.137403237079391, 4.1344182789425235, 4.13714101574282, 2.4283270692294625, 2.4756868718811034, 4.018356656816601, 4.075823226764818, 4.132870757757657, 4.1328645215282105, 4.017475553309175, 4.027807106406766, 4.038152601171549, 4.037898495625131, 4.037720323554845, 4.075189529562416, 4.0855258140795625, 4.088534193106342, 4.088533897427158, 4.132858983863129, 10.64658002858416, 4.138835323826395, 7.450181148687534, 7.337674177830209, 7.236861090466318, 4.138821750476223, 4.138688667190026, 4.132859193542328, 4.132858983863129, 4.132857584501136, 4.1328645215282105, 4.132870757757657, 2.4756868718811034, 7.450181148687534, 10.64658002858416, 4.0855258140795625, 4.075189529562416, 4.075823226764818, 4.1344182789425235, 2.4283270692294625, 4.027807106406766, 4.038152601171549, 4.138820450010967, 4.138821750476223, 4.017475553309175, 4.018356656816601, 4.037720323554845, 4.037898495625131, 4.0765243588526845, 4.086869581198622, 4.086869301757474, 4.088534193106342, 4.088533897427158, 4.135709647065291, 4.135976808903091, 4.135755219680921, 4.137403296105969, 4.137403237079391, 7.236861090466318, 7.453210626834369, 7.404434534298611, 7.337674177830209, 4.137402361692019, 4.037720323554845, 4.037898495625131, 4.038152601171549, 2.4283270692294625, 7.236861090466318, 4.027807106406766, 4.086869301757474, 4.086869581198622, 4.0855258140795625, 4.088533897427158, 4.088534193106342, 7.404434534298611, 2.4756868718811034, 4.0765243588526845, 4.135755219680921, 4.017475553309175, 4.018356656816601, 4.075823226764818, 4.075189529562416, 4.132870757757657, 4.132857584501136, 4.1328645215282105, 4.132858983863129, 4.132859193542328, 4.1344182789425235, 4.135709647065291, 4.135976808903091, 4.13714101574282, 4.137402361692019, 4.137402061261573, 4.138820450010967, 4.138834467386143, 4.138821750476223, 4.137403296105969, 7.453210626834369, 4.138688667190026, 10.64658002858416, 7.450181148687534, 7.337674177830209, 4.017475553309175, 4.018356656816601, 7.236861090466318, 7.337674177830209, 4.027807106406766, 4.075189529562416, 4.0765243588526845, 4.075823226764818, 10.64658002858416, 2.4283270692294625, 2.4756868718811034, 4.037898495625131, 4.135709647065291, 4.13714101574282, 4.037720323554845, 4.038152601171549, 4.138688667190026, 4.0855258140795625, 4.086869581198622, 4.086869301757474, 4.088534193106342, 4.088533897427158, 4.132870757757657, 4.1328645215282105, 4.132857584501136, 4.132858983863129, 4.132859193542328, 4.1344182789425235, 4.135755219680921, 4.135976808903091, 4.137402061261573, 4.138820450010967, 4.138834467386143, 4.138835323826395, 4.138821750476223, 7.450181148687534, 7.453210626834369, 7.404434534298611, 4.137403237079391], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -2.5246999263763428, -2.5246999263763428, -2.5246999263763428, -2.5257999897003174, -2.5267999172210693, -2.5267999172210693, -2.525599956512451, -3.1666998863220215, -3.1666998863220215, -3.171099901199341, -3.171299934387207, -3.171299934387207, -3.171299934387207, -3.171299934387207, -3.1738998889923096, -5.568900108337402, -5.568900108337402, -5.5258002281188965, -5.5258002281188965, -5.525199890136719, -5.525599956512451, -5.568699836730957, -5.568600177764893, -5.568699836730957, -5.568699836730957, -5.568600177764893, -5.56850004196167, -5.568600177764893, -5.568600177764893, -5.56850004196167, -5.524799823760986, -5.524700164794922, -5.567200183868408, -5.5680999755859375, -5.56850004196167, -5.56850004196167, -1.8568999767303467, -2.526900053024292, -2.526900053024292, -2.526900053024292, -2.135999917984009, -3.1710000038146973, -3.1710000038146973, -3.1751999855041504, -3.1714999675750732, -3.1714999675750732, -3.1714999675750732, -3.1714999675750732, -3.175600051879883, -3.175600051879883, -5.568999767303467, -5.568999767303467, -5.430300235748291, -5.428699970245361, -5.483799934387207, -5.525700092315674, -5.56879997253418, -5.568699836730957, -5.568699836730957, -5.56879997253418, -5.568900108337402, -5.56879997253418, -5.568699836730957, -5.568699836730957, -5.56879997253418, -5.56879997253418, -5.443299770355225, -5.568600177764893, -5.568299770355225, -5.568399906158447, -5.56850004196167, -5.568699836730957, -5.568699836730957, -2.5241000652313232, -2.5241000652313232, -2.524199962615967, -2.5262999534606934, -2.52839994430542, -3.170799970626831, -2.133699893951416, -1.8566999435424805, -3.166300058364868, -3.1703999042510986, -3.174499988555908, -3.1707000732421875, -5.525300025939941, -5.525100231170654, -5.525199890136719, -5.524899959564209, -5.525000095367432, -5.56820011138916, -5.5680999755859375, -5.5680999755859375, -5.5680999755859375, -5.567999839782715, -5.567800045013428, -5.567999839782715, -5.567999839782715, -5.5680999755859375, -5.567999839782715, -5.567999839782715, -5.56820011138916, -5.56790018081665, -5.567999839782715, -5.567399978637695, -5.567500114440918, -5.567599773406982, -5.567699909210205, -5.567999839782715, -2.2618000507354736, -2.2660000324249268, -2.2660000324249268, -2.9124999046325684, -2.2660999298095703, -2.9124999046325684, -2.908900022506714, -2.908900022506714, -2.9130001068115234, -2.9130001068115234, -2.913100004196167, -2.9107000827789307, -5.306000232696533, -5.262700080871582, -5.262800216674805, -5.305699825286865, -5.305699825286865, -5.305500030517578, -5.305799961090088, -5.305699825286865, -5.305799961090088, -5.305799961090088, -5.3059000968933105, -5.3059000968933105, -5.305500030517578, -5.305799961090088, -5.305799961090088, -5.305699825286865, -5.305699825286865, -5.305799961090088, -5.305600166320801, -5.305699825286865, -5.305699825286865, -5.305799961090088, -5.262400150299072, -5.305799961090088, -5.3043999671936035, -5.305300235748291, -5.305500030517578, -2.1589999198913574, -2.166100025177002, -2.1568000316619873, -2.1579999923706055, -2.805500030517578, -2.805999994277954, -2.8059000968933105, -2.8155999183654785, -2.1621999740600586, -5.203199863433838, -5.2032999992370605, -5.159999847412109, -5.159900188446045, -5.1596999168396, -5.202899932861328, -5.202899932861328, -5.179100036621094, -5.202899932861328, -5.202899932861328, -5.203100204467773, -5.202899932861328, -5.203000068664551, -5.2027997970581055, -5.2027997970581055, -5.202899932861328, -5.202899932861328, -5.203000068664551, -5.2027997970581055, -5.203000068664551, -5.203000068664551, -5.2027997970581055, -5.2027997970581055, -5.2027997970581055, -5.2027997970581055, -5.202899932861328, -5.202199935913086, -5.202400207519531, -5.202499866485596, -5.202899932861328], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.3124, 1.3124, 1.3124, 1.3113, 1.3103, 1.3103, 0.7388, 0.6826, 0.6826, 0.6664, 0.6662, 0.6662, 0.6662, 0.6662, 0.0753, -1.1986, -1.2179, -1.664, -1.6641, -1.687, -1.6878, -1.7018, -1.702, -1.7044, -1.7068, -1.716, -1.7161, -1.7163, -1.7185, -1.7188, -2.2465, -2.2693, -2.6749, -2.3193, -1.7307, -1.73, 1.392, 1.3109, 1.3109, 1.3108, 1.1194, 0.6787, 0.6787, 0.6771, 0.666, 0.666, 0.666, 0.6659, 0.6626, 0.6618, -1.1987, -1.218, -1.5637, -1.5762, -1.6453, -1.6871, -1.7019, -1.7044, -1.707, -1.707, -1.7071, -1.7162, -1.7186, -1.7194, -1.7195, -1.7302, -2.5511, -1.7316, -2.3191, -2.3039, -2.2902, -1.7316, -1.7316, 1.3144, 1.3144, 1.3144, 1.3123, 1.3101, 1.1802, 1.1155, 1.0356, 0.6838, 0.6822, 0.6779, 0.6674, -1.155, -1.6608, -1.6635, -1.6879, -1.6879, -1.7013, -1.7015, -1.7063, -1.7063, -1.7157, -1.7181, -1.7183, -1.7187, -1.7188, -1.7302, -1.7302, -1.7304, -1.7305, -1.7305, -2.2891, -2.3187, -2.3122, -2.3032, -1.7306, 1.6, 1.5957, 1.5957, 1.4578, 1.0122, 0.9517, 0.9409, 0.9408, 0.9371, 0.9363, 0.9363, 0.3447, -0.955, -1.4104, -1.425, -1.4389, -1.4391, -1.4531, -1.4532, -1.4671, -1.4673, -1.4673, -1.4674, -1.4674, -1.4674, -1.468, -1.4681, -1.4682, -1.4683, -1.4683, -1.4685, -1.4686, -1.4687, -1.4684, -2.0136, -1.4687, -2.4121, -2.056, -2.0411, 1.7078, 1.7005, 1.1215, 1.1065, 1.0587, 1.0466, 1.0464, 1.0368, 0.73, -0.8329, -0.8523, -1.2982, -1.3221, -1.3222, -1.341, -1.3412, -1.342, -1.3529, -1.3532, -1.3533, -1.3536, -1.3537, -1.3643, -1.3643, -1.3643, -1.3644, -1.3645, -1.3646, -1.3652, -1.3652, -1.3654, -1.3657, -1.3657, -1.3658, -1.3658, -1.953, -1.9536, -1.9471, -1.3655]}, \"token.table\": {\"Topic\": [4, 2, 4, 1, 2, 1, 2, 1, 4, 3, 1, 2, 1, 3, 5, 2, 2, 1, 2, 2, 4, 2, 3, 4, 5, 5, 1, 3, 5, 1, 4, 2, 5, 3, 5, 1, 5, 3, 2, 4, 2, 1, 4, 4, 5, 3, 3, 4, 3, 1, 4, 3, 1, 2, 3, 4, 3, 5, 1, 1, 1, 2], \"Freq\": [0.7429935110906277, 0.48937214584766414, 0.48937214584766414, 0.48342562953245183, 0.48342562953245183, 0.48339509314295603, 0.48339509314295603, 0.4891728686951403, 0.4891728686951403, 0.7258897561503039, 0.4833951282438723, 0.4833951282438723, 0.7248439727211767, 0.6574881305739735, 0.2817806273888458, 0.9391925641813155, 0.7253894146386235, 0.48339498397034564, 0.48339498397034564, 0.4893721123866713, 0.4893721123866713, 0.7253425583872254, 0.8078566084895616, 0.49654810847786934, 0.49654810847786934, 0.7465738500117713, 0.26844984841104585, 0.6711246210276146, 0.7467375868731584, 0.4891729040716929, 0.4891729040716929, 0.490614019184443, 0.490614019184443, 0.4906984156885279, 0.4906984156885279, 0.4088488977970833, 0.4088488977970833, 0.7258887835236102, 0.6752710118293493, 0.27010840473173975, 0.7253814214448248, 0.7248413589457763, 0.7429607265389052, 0.4145443670256617, 0.4145443670256617, 0.7258897193225353, 0.4895330713876751, 0.4895330713876751, 0.7258876882053019, 0.7248672807362575, 0.8236122824404474, 0.725890001932433, 0.7248442004755365, 0.4837439913098362, 0.4837439913098362, 0.7429139748531642, 0.49077471992198485, 0.49077471992198485, 0.7248417455783469, 0.7248415955884105, 0.483394990866737, 0.483394990866737], \"Term\": [\"bag\", \"of\", \"of\", \"sklearn\", \"sklearn\", \"words\", \"words\", \"\\u0431\\u043b\\u0438\\u0436\\u0435\", \"\\u0431\\u043b\\u0438\\u0436\\u0435\", \"\\u0434\\u0430\\u043d\\u043d\\u043e\\u0435\", \"\\u0434\\u0435\\u043b\\u0438\\u0442\\u044c\", \"\\u0434\\u0435\\u043b\\u0438\\u0442\\u044c\", \"\\u0434\\u0435\\u0442\\u0430\\u043b\\u0438\", \"\\u0434\\u043e\\u043a\\u0443\\u043c\\u0435\\u043d\\u0442\\u043e\\u0432\", \"\\u0434\\u043e\\u043a\\u0443\\u043c\\u0435\\u043d\\u0442\\u043e\\u0432\", \"\\u0435\\u0434\\u0438\\u043d\\u0438\\u0447\\u043a\\u0430\", \"\\u0435\\u0449\\u0451\", \"\\u0437\\u043d\\u0430\\u043c\\u0435\\u043d\\u0430\\u0442\\u0435\\u043b\\u044e\", \"\\u0437\\u043d\\u0430\\u043c\\u0435\\u043d\\u0430\\u0442\\u0435\\u043b\\u044e\", \"\\u0437\\u043d\\u0430\\u0447\\u0435\\u043d\\u0438\\u0435\", \"\\u0437\\u043d\\u0430\\u0447\\u0435\\u043d\\u0438\\u0435\", \"\\u0438\\u0441\\u0445\\u043e\\u0434\\u043d\\u043e\\u0435\", \"\\u043a\\u0430\\u0436\\u0434\\u043e\\u0433\\u043e\", \"\\u043a\\u0430\\u0436\\u0434\\u043e\\u0439\", \"\\u043a\\u0430\\u0436\\u0434\\u043e\\u0439\", \"\\u043a\\u043e\\u043b\\u0438\\u0447\\u0435\\u0441\\u0442\\u0432\\u0430\", \"\\u043a\\u043e\\u043b\\u0438\\u0447\\u0435\\u0441\\u0442\\u0432\\u0443\", \"\\u043a\\u043e\\u043b\\u0438\\u0447\\u0435\\u0441\\u0442\\u0432\\u0443\", \"\\u043b\\u043e\\u0433\\u0430\\u0440\\u0438\\u0444\\u043c\", \"\\u043b\\u043e\\u0433\\u0430\\u0440\\u0438\\u0444\\u043c\\u0443\", \"\\u043b\\u043e\\u0433\\u0430\\u0440\\u0438\\u0444\\u043c\\u0443\", \"\\u043d\\u043e\\u0440\\u043c\\u0430\\u043b\\u0438\\u0437\\u0430\\u0446\\u0438\\u044f\", \"\\u043d\\u043e\\u0440\\u043c\\u0430\\u043b\\u0438\\u0437\\u0430\\u0446\\u0438\\u044f\", \"\\u043e\\u0431\\u0449\\u0435\\u0433\\u043e\", \"\\u043e\\u0431\\u0449\\u0435\\u0433\\u043e\", \"\\u043e\\u043d\\u043e\", \"\\u043e\\u043d\\u043e\", \"\\u043e\\u0442\\u043d\\u043e\\u0448\\u0435\\u043d\\u0438\\u0435\", \"\\u043f\\u0440\\u0438\\u0431\\u0430\\u0432\\u043b\\u044f\\u0435\\u0442\\u0441\\u044f\", \"\\u043f\\u0440\\u0438\\u0431\\u0430\\u0432\\u043b\\u044f\\u0435\\u0442\\u0441\\u044f\", \"\\u043f\\u0440\\u043e\\u0432\\u043e\\u0434\\u0438\\u0442\\u0441\\u044f\", \"\\u0440\\u0430\\u0432\\u043d\\u043e\\u043c\\u0443\", \"\\u0440\\u0435\\u0434\\u043a\\u0438\\u0445\", \"\\u0441\\u043b\\u043e\\u0432\", \"\\u0441\\u043b\\u043e\\u0432\", \"\\u0441\\u043b\\u043e\\u0432\\u0430\", \"\\u0441\\u043b\\u043e\\u0432\\u043e\", \"\\u0441\\u043b\\u043e\\u0432\\u043e\", \"\\u0441\\u043e\\u0434\\u0435\\u0440\\u0436\\u0430\\u0449\\u0438\\u0445\", \"\\u0441\\u0442\\u0440\\u0435\\u043c\\u0438\\u0442\\u044c\\u0441\\u044f\", \"\\u0441\\u0442\\u0440\\u043e\\u043a\\u0438\", \"\\u0441\\u0447\\u0438\\u0442\\u0430\\u0435\\u0442\\u0441\\u044f\", \"\\u0442\\u0435\\u0445\\u043d\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0435\", \"\\u0443\\u043c\\u043d\\u043e\\u0436\\u0430\\u0435\\u0442\\u0441\\u044f\", \"\\u0443\\u043c\\u043d\\u043e\\u0436\\u0430\\u0435\\u0442\\u0441\\u044f\", \"\\u0447\\u0430\\u0441\\u0442\\u044b\\u0445\", \"\\u0447\\u0438\\u0441\\u043b\\u0430\", \"\\u0447\\u0438\\u0441\\u043b\\u0430\", \"\\u0447\\u0438\\u0441\\u043b\\u0438\\u0442\\u0435\\u043b\\u044e\", \"\\u0447\\u0438\\u0441\\u043b\\u0443\", \"\\u044d\\u0442\\u043e\", \"\\u044d\\u0442\\u043e\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [4, 2, 5, 3, 1]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el4211398335679694888963386457\", ldavis_el4211398335679694888963386457_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el4211398335679694888963386457\", ldavis_el4211398335679694888963386457_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el4211398335679694888963386457\", ldavis_el4211398335679694888963386457_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ],
            "text/plain": [
              "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
              "topic                                                \n",
              "3     -0.106961 -0.180802       1        1  22.671934\n",
              "1     -0.184314 -0.001597       2        1  22.655564\n",
              "4      0.228425 -0.062956       3        1  22.618924\n",
              "2     -0.062045  0.192021       4        1  16.900551\n",
              "0      0.124895  0.053335       5        1  15.153026, topic_info=            Term       Freq      Total Category  logprob  loglift\n",
              "6     документов  10.000000  10.000000  Default  30.0000  30.0000\n",
              "30      единичка   7.000000   7.000000  Default  29.0000  29.0000\n",
              "12          слов   7.000000   7.000000  Default  28.0000  28.0000\n",
              "13           оно   7.000000   7.000000  Default  27.0000  27.0000\n",
              "19      логарифм   4.000000   4.000000  Default  26.0000  26.0000\n",
              "..           ...        ...        ...      ...      ...      ...\n",
              "35        детали   0.160039   4.138822   Topic5  -5.2029  -1.3658\n",
              "7     количеству   0.160141   7.450181   Topic5  -5.2022  -1.9530\n",
              "30      единичка   0.160110   7.453211   Topic5  -5.2024  -1.9536\n",
              "29  прибавляется   0.160091   7.404435   Topic5  -5.2025  -1.9471\n",
              "33           это   0.160031   4.137403   Topic5  -5.2029  -1.3655\n",
              "\n",
              "[217 rows x 6 columns], token_table=      Topic      Freq       Term\n",
              "term                            \n",
              "24        4  0.742994        bag\n",
              "25        2  0.489372         of\n",
              "25        4  0.489372         of\n",
              "36        1  0.483426    sklearn\n",
              "36        2  0.483426    sklearn\n",
              "...     ...       ...        ...\n",
              "20        5  0.490775      числа\n",
              "27        1  0.724842  числителю\n",
              "17        1  0.724842      числу\n",
              "33        1  0.483395        это\n",
              "33        2  0.483395        это\n",
              "\n",
              "[62 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[4, 2, 5, 3, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}