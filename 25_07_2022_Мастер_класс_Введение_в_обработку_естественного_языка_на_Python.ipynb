{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMyGnxHgJ3DrguKm8wERjQV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RusAl84/text_mining/blob/master/25_07_2022_%D0%9C%D0%B0%D1%81%D1%82%D0%B5%D1%80_%D0%BA%D0%BB%D0%B0%D1%81%D1%81_%D0%92%D0%B2%D0%B5%D0%B4%D0%B5%D0%BD%D0%B8%D0%B5_%D0%B2_%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D1%83_%D0%B5%D1%81%D1%82%D0%B5%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D0%BE%D0%B3%D0%BE_%D1%8F%D0%B7%D1%8B%D0%BA%D0%B0_%D0%BD%D0%B0_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klN_zix5Vqir"
      },
      "source": [
        "#Подход TF-IDF\n",
        "\n",
        "У подхода bag-of-words есть существенный недостаток. Если слово встречается 5 раз в конкретном документе, но и в других документах тоже встречается часто, то его наличие в документе не особо-то о чём-то говорит. Если же слово 5 раз встречается в конкретном документе, но в других документах встречается редко, то его наличие (да ещё и многократное) позволяет хорошо отличать этот документ от других. Однако с точки зрения bag-of-words различий не будет: в обеих ячейках будет просто число 5.\n",
        "\n",
        "Отчасти это решается исключением стоп-слов (и слишком часто встречающихся слов), но лишь отчасти. Другой идеей является отмасштабировать получившуюся таблицу с учётом \"редкости\" слова в наборе документов (т.е. с учётом информативности слова).\n",
        "\n",
        "tfidf=tf∗idf\n",
        "\n",
        "idf=log((N+1)/(Nw+1))+1\n",
        "\n",
        "Здесь tf это частота слова в тексте (то же самое, что в bag of words), N - общее число документов, Nw - число документов, содержащих данное слово.\n",
        "\n",
        "То есть для каждого слова считается отношение общего количества документов к количеству документов, содержащих данное слово (для частых слов оно будет ближе к 1, для редких слов оно будет стремиться к числу, равному количеству документов), и на логарифм от этого числа умножается исходное значение bag-of-words (к числителю и знаменателю прибавляется единичка, чтобы не делить на 0, и к логарифму тоже прибавляется единичка, но это уже технические детали). После этого в sklearn ещё проводится L2-нормализация каждой строки.\n",
        "\n",
        "В sklearn есть класс для поддержки TF-IDF: TfidfVectorizer, рассмотрим его."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rl9lfwIGyuZ",
        "outputId": "2a15db2f-d597-4b66-85ea-ade31ceae189"
      },
      "source": [
        "texts = \"\\u0422\\u043E \\u0435\\u0441\\u0442\\u044C \\u044D\\u0442\\u043E  \\u043E\\u0442\\u043A\\u0440\\u044B\\u0432\\u0430\\u0435\\u0442 \\u0431\\u043E\\u043B\\u044C\\u0448\\u0438\\u0435 \\u043F\\u0435\\u0440\\u0441\\u043F\\u0435\\u043A\\u0442\\u0438\\u0432\\u044B. \\u0412 PostgreSQL \\u0434\\u0435\\u0439\\u0441\\u0442\\u0432\\u0438\\u0442\\u0435\\u043B\\u044C\\u043D\\u043E \\u0432\\u043F\\u043E\\u043B\\u043D\\u0435 \\u043C\\u043E\\u0436\\u043D\\u043E  \\u0441\\u0442\\u0440\\u0438\\u043C\\u0438\\u0442\\u044C, \\u0435\\u0441\\u043B\\u0438 \\u0438\\u0441\\u043F\\u043E\\u043B\\u044C\\u0437\\u043E\\u0432\\u0430\\u0442\\u044C \\u043D\\u0430\\u0448\\u0438 \\u043E\\u043F\\u0442\\u0438\\u043C\\u0438\\u0437\\u0430\\u0446\\u0438\\u0438.  \\u0417\\u0434\\u0435\\u0441\\u044C \\u043F\\u043E\\u043A\\u0430\\u0437\\u0430\\u043D\\u043E, \\u043A\\u0430\\u043A \\u0440\\u0430\\u0441\\u0442\\u0435\\u0442 \\u0440\\u0430\\u0437\\u043C\\u0435\\u0440 WAL. \\u0421\\u043F\\u0440\\u0430\\u0432\\u0430 \\u043E\\u043D \\u0437\\u0430\\u0432\\u0438\\u0441\\u0438\\u0442 \\u0442\\u043E\\u043B\\u044C\\u043A\\u043E \\u043E\\u0442 \\u0440\\u0430\\u0437\\u043C\\u0435\\u0440\\u0430 \\u0434\\u043E\\u0431\\u0430\\u0432\\u043B\\u044F\\u0435\\u043C\\u043E\\u0433\\u043E \\u043A\\u0443\\u0441\\u043A\\u0430, \\u0430 \\u0441\\u043B\\u0435\\u0432\\u0430 \\u2014 \\u043E\\u0442 \\u0441\\u0430\\u043C\\u043E\\u0439 \\u0441\\u0442\\u0440\\u043E\\u043A\\u0438.   \\u041C\\u044B \\u043F\\u043E\\u0441\\u0447\\u0438\\u0442\\u0430\\u043B\\u0438 \\u0441\\u043A\\u043E\\u0440\\u043E\\u0441\\u0442\\u044C (\\u041C\\u0431/\\u0441) \\u0434\\u043B\\u044F \\u043E\\u0440\\u0438\\u0433\\u0438\\u043D\\u0430\\u043B\\u044C\\u043D\\u043E\\u0433\\u043E \\u043F\\u043E\\u0441\\u0442\\u0433\\u0440\\u0435\\u0441\\u0430 (\\u0441\\u043B\\u0435\\u0432\\u0430) \\u0438 \\u0441 \\u043E\\u043F\\u0442\\u0438\\u043C\\u0438\\u0437\\u0430\\u0446\\u0438\\u0435\\u0439.  \\u0421\\u043B\\u0435\\u0432\\u0430 \\u043C\\u044B \\u043E\\u0433\\u0440\\u0430\\u043D\\u0438\\u0447\\u0438\\u043B\\u0438\\u0441\\u044C \\u0441\\u043B\\u0443\\u0447\\u0430\\u0435\\u043C, \\u043A\\u043E\\u0433\\u0434\\u0430 \\u043C\\u044B \\u0434\\u043E\\u0431\\u0430\\u0432\\u043B\\u044F\\u0435\\u043C  10-\\u0431\\u0430\\u0439\\u0442\\u043D\\u044B\\u0435 \\u043A\\u0443\\u0441\\u043E\\u0447\\u043A\\u0438, \\u0438\\u043D\\u0430\\u0447\\u0435 \\u0432\\u0441\\u0451 \\u0431\\u0443\\u0434\\u0435\\u0442 \\u0441\\u043B\\u0438\\u0448\\u043A\\u043E\\u043C \\u043C\\u0435\\u0434\\u043B\\u0435\\u043D\\u043D\\u043E. \\u041C\\u044B \\u0432\\u0438\\u0434\\u0438\\u043C, \\u0447\\u0442\\u043E \\u0441\\u043A\\u043E\\u0440\\u043E\\u0441\\u0442\\u044C \\u043E\\u0447\\u0435\\u043D\\u044C \\u0431\\u044B\\u0441\\u0442\\u0440\\u043E \\u043F\\u0430\\u0434\\u0430\\u0435\\u0442 \\u0441  1 \\u041C\\u0431/\\u0441 \\u0434\\u043E 1 \\u041A\\u0431/\\u0441, \\u043F\\u043E\\u043B\\u043E\\u0441\\u0430 \\u0441\\u043E\\u0432\\u0441\\u0435\\u043C \\u043C\\u0430\\u043B\\u0435\\u043D\\u044C\\u043A\\u0430\\u044F. \\u0421\\u043F\\u0440\\u0430\\u0432\\u0430 \\u043C\\u044B \\u0432\\u0438\\u0434\\u0438\\u043C, \\u0447\\u0442\\u043E  \\u043F\\u0440\\u043E\\u0438\\u0437\\u0432\\u043E\\u0434\\u0438\\u0442\\u0435\\u043B\\u044C\\u043D\\u043E\\u0441\\u0442\\u044C \\u0434\\u043E\\u043F\\u0438\\u0441\\u044B\\u0432\\u0430\\u043D\\u0438\\u044F  \\u043D\\u0435 \\u0437\\u0430\\u0432\\u0438\\u0441\\u0438\\u0442 \\u043E\\u0442 \\u0440\\u0430\\u0437\\u043C\\u0435\\u0440\\u0430 \\u0434\\u0430\\u043D\\u043D\\u044B\\u0445 \\u0434\\u043B\\u044F \\u043B\\u044E\\u0431\\u044B\\u0445 \\u0440\\u0430\\u0437\\u043C\\u0435\\u0440\\u043E\\u0432 \\u0434\\u043E\\u0431\\u0430\\u0432\\u043B\\u0435\\u043D\\u0438\\u0439, \\u0438 \\u043C\\u043E\\u0436\\u043D\\u043E \\u043F\\u0440\\u0435\\u0434\\u0441\\u043A\\u0430\\u0437\\u0430\\u0442\\u044C, \\u0447\\u0442\\u043E \\u0435\\u0441\\u043B\\u0438 \\u043C\\u044B \\u0445\\u043E\\u0442\\u0438\\u043C \\u0437\\u0430\\u043D\\u044F\\u0442\\u044C \\u043F\\u043E\\u043B\\u043E\\u0441\\u0443 20 \\u041C\\u0431/\\u0441, \\u0442\\u043E \\u043D\\u0430\\u0434\\u043E \\u0430\\u043F\\u0434\\u0435\\u0439\\u0442\\u0438\\u0442\\u044C \\u043A\\u0438\\u043B\\u043E\\u0431\\u0430\\u0439\\u0442\\u043D\\u044B\\u043C\\u0438 \\u0447\\u0430\\u043D\\u043A\\u0430\\u043C\\u0438 \\u0438 \\u0434\\u0435\\u0433\\u0440\\u0430\\u0434\\u0430\\u0446\\u0438\\u0438 \\u043F\\u0440\\u043E\\u0438\\u0437\\u0432\\u043E\\u0434\\u0438\\u0442\\u0435\\u043B\\u044C\\u043D\\u043E\\u0441\\u0442\\u0438 \\u043D\\u0435 \\u0431\\u0443\\u0434\\u0435\\u0442.   \\u0412\\u043C\\u0435\\u0441\\u0442\\u043E \\u0417\\u0430\\u043A\\u043B\\u044E\\u0447\\u0435\\u043D\\u0438\\u044F \\u0412 \\u044D\\u0442\\u043E\\u0439 \\u0441\\u0435\\u0440\\u0438\\u0438 \\u0441\\u0442\\u0430\\u0442\\u0435\\u0439 \\u044F \\u0440\\u0430\\u0441\\u0441\\u043A\\u0430\\u0437\\u0430\\u043B \\u043F\\u0440\\u043E \\u0432\\u043E\\u0437\\u043C\\u043E\\u0436\\u043D\\u043E\\u0441\\u0442\\u0438 \\u0443\\u043B\\u0443\\u0447\\u0448\\u0435\\u043D\\u0438\\u044F PostgreSQL \\u0434\\u043B\\u044F \\u044D\\u0444\\u0444\\u0435\\u043A\\u0442\\u0438\\u0432\\u043D\\u043E\\u0433\\u043E \\u0445\\u0440\\u0430\\u043D\\u0435\\u043D\\u0438\\u044F \\u0431\\u043E\\u043B\\u044C\\u0448\\u0438\\u0445 \\u0437\\u043D\\u0430\\u0447\\u0435\\u043D\\u0438\\u0439 \\u043D\\u0430 \\u043F\\u0440\\u0438\\u043C\\u0435\\u0440\\u0435 \\u043F\\u043E\\u043F\\u0443\\u043B\\u044F\\u0440\\u043D\\u043E\\u0433\\u043E \\u0442\\u0438\\u043F\\u0430 \\u0434\\u0430\\u043D\\u043D\\u044B\\u0445 JSONB \\u0438 \\u0431\\u044B\\u0441\\u0442\\u0440\\u043E\\u0433\\u043E \\u0434\\u043E\\u043F\\u0438\\u0441\\u044B\\u0432\\u0430\\u043D\\u0438\\u044F \\u0431\\u0438\\u043D\\u0430\\u0440\\u043D\\u044B\\u0445 \\u0434\\u0430\\u043D\\u043D\\u044B\\u0445. PostgreSQL \\u0441\\u043B\\u0430\\u0432\\u0438\\u0442\\u0441\\u044F \\u0441\\u0432\\u043E\\u0435\\u0439 \\u0440\\u0430\\u0441\\u0448\\u0438\\u0440\\u044F\\u0435\\u043C\\u043E\\u0441\\u0442\\u044C\\u044E, \\u043F\\u043E\\u044D\\u0442\\u043E\\u043C\\u0443 \\u043B\\u043E\\u0433\\u0438\\u0447\\u043D\\u043E \\u0435\\u0435 \\u0440\\u0430\\u0441\\u0448\\u0438\\u0440\\u0438\\u0442\\u044C \\u0438 \\u043D\\u0430 TOAST \\u2014 \\u0442\\u0430\\u043A \\u0447\\u0442\\u043E\\u0431\\u044B \\u0445\\u0440\\u0430\\u043D\\u0435\\u043D\\u0438\\u0435 \\u0431\\u043E\\u043B\\u044C\\u0448\\u0438\\u0445 \\u0437\\u043D\\u0430\\u0447\\u0435\\u043D\\u0438\\u0439 \\u0431\\u044B\\u043B\\u043E datatype aware.  \\u041C\\u044B \\u043F\\u0440\\u0435\\u0434\\u043B\\u043E\\u0436\\u0438\\u043B\\u0438 \\u0441\\u0435\\u0440\\u0438\\u044E \\u043F\\u0430\\u0442\\u0447\\u0435\\u0439, \\u0440\\u0435\\u0430\\u043B\\u0438\\u0437\\u0443\\u044E\\u0449\\u0438\\u0445 API \\u0434\\u043B\\u044F TOAST (\\u0441\\u043C. Pluggable TOAST), \\u043D\\u0430 \\u043E\\u0441\\u043D\\u043E\\u0432\\u0435 \\u043A\\u043E\\u0442\\u043E\\u0440\\u043E\\u0433\\u043E \\u043C\\u043E\\u0436\\u043D\\u043E \\u0440\\u0430\\u0437\\u0440\\u0430\\u0431\\u0430\\u0442\\u044B\\u0432\\u0430\\u0442\\u044C TOAST, \\u043E\\u043F\\u0442\\u0438\\u043C\\u0438\\u0437\\u0438\\u0440\\u043E\\u0432\\u0430\\u043D\\u043D\\u044B\\u0439 \\u0434\\u043B\\u044F \\u043E\\u043F\\u0440\\u0435\\u0434\\u0435\\u043B\\u0435\\u043D\\u043D\\u043E\\u0433\\u043E \\u0442\\u0438\\u043F\\u0430 \\u0434\\u0430\\u043D\\u043D\\u044B\\u0445. \\u041D\\u0430\\u043F\\u0440\\u0438\\u043C\\u0435\\u0440, \\u0432\\u0441\\u0435 \\u043E\\u043F\\u0438\\u0441\\u0430\\u043D\\u043D\\u044B\\u0435 \\u043E\\u043F\\u0442\\u0438\\u043C\\u0438\\u0437\\u0430\\u0446\\u0438\\u0438 \\u0434\\u043B\\u044F JSONB \\u043C\\u043E\\u0436\\u043D\\u043E \\u0440\\u0435\\u0430\\u043B\\u0438\\u0437\\u043E\\u0432\\u0430\\u0442\\u044C \\u0432 \\u0432\\u0438\\u0434\\u0435 \\u0440\\u0430\\u0441\\u0448\\u0438\\u0440\\u0435\\u043D\\u0438\\u044F. \\u041D\\u0430\\u0434\\u0435\\u0435\\u043C\\u0441\\u044F \\u0437\\u0430\\u043A\\u043E\\u043C\\u043C\\u0438\\u0442\\u0438\\u0442\\u044C \\u0432\\u0441\\u0435 \\u044D\\u0442\\u043E \\u0434\\u043B\\u044F \\u0441\\u043B\\u0435\\u0434\\u0443\\u044E\\u0449\\u0435\\u0439 \\u0432\\u0435\\u0440\\u0441\\u0438\\u0438 PG15.  \\u0412\\u0438\\u0434\\u0435\\u043E \\u043C\\u043E\\u0435\\u0433\\u043E \\u0432\\u044B\\u0441\\u0442\\u0443\\u043F\\u043B\\u0435\\u043D\\u0438\\u044F \\u043D\\u0430 Saint HighLoad++ 2021:   \\u041A\\u043E\\u043D\\u0444\\u0435\\u0440\\u0435\\u043D\\u0446\\u0438\\u044F HighLoad++ Foundation 2022 \\u043F\\u0440\\u043E\\u0439\\u0434\\u0435\\u0442 17 \\u0438 18 \\u043C\\u0430\\u0440\\u0442\\u0430 \\u0432 \\u041C\\u043E\\u0441\\u043A\\u0432\\u0435 \\u0432 \\u041A\\u0440\\u043E\\u043A\\u0443\\u0441 \\u042D\\u043A\\u0441\\u043F\\u043E. \\u2014 1 \\u0444\\u0435\\u0432\\u0440\\u0430\\u043B\\u044F. \\u041F\\u043B\\u0430\\u043D\\u0438\\u0440\\u0443\\u0439\\u0442\\u0435 \\u0441\\u0432\\u043E\\u0435 \\u0443\\u0447\\u0430\\u0441\\u0442\\u0438\\u0435, \\u0440\\u0430\\u0441\\u043F\\u0438\\u0441\\u0430\\u043D\\u0438\\u0435 \\u0438 \\u043F\\u043E\\u043B\\u043D\\u044B\\u0439 \\u0441\\u043F\\u0438\\u0441\\u043E\\u043A \\u0442\\u0435\\u043C \\u0441 \\u0442\\u0435\\u0437\\u0438\\u0441\\u0430\\u043C\\u0438 \\u0443\\u0436\\u0435 \\u043D\\u0430 \\u0441\\u0430\\u0439\\u0442\\u0435.  \\u041D\\u0430\\u0448 \\u0434\\u043E\\u043A\\u043B\\u0430\\u0434 \\u0441 \\u043A\\u043E\\u043B\\u043B\\u0435\\u0433\\u0430\\u043C\\u0438 \\u2014 Pluggable TOAST or One TOAST fits ALL \\u2014 \\u0431\\u0443\\u0434\\u0435\\u0442 \\u043B\\u043E\\u0433\\u0438\\u0447\\u043D\\u044B\\u043C \\u043F\\u0440\\u043E\\u0434\\u043E\\u043B\\u0436\\u0435\\u043D\\u0438\\u0435\\u043C \\u0442\\u043E\\u0433\\u043E, \\u0447\\u0442\\u043E \\u044F \\u0440\\u0430\\u0441\\u0441\\u043A\\u0430\\u0437\\u0430\\u043B \\u0432 \\u044D\\u0442\\u043E\\u0439 \\u0441\\u0435\\u0440\\u0438\\u0438 \\u0441\\u0442\\u0430\\u0442\\u0435\\u0439. \\u0411\\u0438\\u043B\\u0435\\u0442\\u044B \\u043C\\u043E\\u0436\\u043D\\u043E \\u043A\\u0443\\u043F\\u0438\\u0442\\u044C \\u0437\\u0434\\u0435\\u0441\\u044C.  \\u0414\\u043E \\u0432\\u0441\\u0442\\u0440\\u0435\\u0447\\u0438 \\u043D\\u0430 \\u043A\\u043E\\u043D\\u0444\\u0435\\u0440\\u0435\\u043D\\u0446\\u0438\\u0438!  \\u0422\\u0435\\u0433\\u0438: postgresqlpostgresjsonjsonbstreamingstream processinghigh performancehighload++postgresql performancetoast \\u0425\\u0430\\u0431\\u044B: \\u0411\\u043B\\u043E\\u0433 \\u043A\\u043E\\u043C\\u043F\\u0430\\u043D\\u0438\\u0438 \\u041A\\u043E\\u043D\\u0444\\u0435\\u0440\\u0435\\u043D\\u0446\\u0438\\u0438 \\u041E\\u043B\\u0435\\u0433\\u0430 \\u0411\\u0443\\u043D\\u0438\\u043D\\u0430 (\\u041E\\u043D\\u0442\\u0438\\u043A\\u043E)\\u0412\\u044B\\u0441\\u043E\\u043A\\u0430\\u044F \\u043F\\u0440\\u043E\\u0438\\u0437\\u0432\\u043E\\u0434\\u0438\\u0442\\u0435\\u043B\\u044C\\u043D\\u043E\\u0441\\u0442\\u044CPostgreSQL\\u0410\\u0434\\u043C\\u0438\\u043D\\u0438\\u0441\\u0442\\u0440\\u0438\\u0440\\u043E\\u0432\\u0430\\u043D\\u0438\\u0435 \\u0431\\u0430\\u0437 \\u0434\\u0430\\u043D\\u043D\\u044B\\u0445\\u0425\\u0440\\u0430\\u043D\\u0438\\u043B\\u0438\\u0449\\u0430 \\u0434\\u0430\\u043D\\u043D\\u044B\\u0445 +33  58 1   \\u041A\\u043E\\u043D\\u0444\\u0435\\u0440\\u0435\\u043D\\u0446\\u0438\\u0438 \\u041E\\u043B\\u0435\\u0433\\u0430 \\u0411\\u0443\\u043D\\u0438\\u043D\\u0430 (\\u041E\\u043D\\u0442\\u0438\\u043A\\u043E) \\u041F\\u0440\\u043E\\u0444\\u0435\\u0441\\u0441\\u0438\\u043E\\u043D\\u0430\\u043B\\u044C\\u043D\\u044B\\u0435 \\u043A\\u043E\\u043D\\u0444\\u0435\\u0440\\u0435\\u043D\\u0446\\u0438\\u0438 \\u0434\\u043B\\u044F IT-\\u0440\\u0430\\u0437\\u0440\\u0430\\u0431\\u043E\\u0442\\u0447\\u0438\\u043A\\u043E\\u0432 \\xD7\\u041F\\u043E\\u0434\\u043F\\u0438\\u0441\\u0430\\u0442\\u044C\\u0441\\u044F FacebookFacebookTwitterTelegram  66 \\u041A\\u0430\\u0440\\u043C\\u0430 0 \\u0420\\u0435\\u0439\\u0442\\u0438\\u043D\\u0433 Oleg Bartunov @zen \\u041F\\u043E\\u043B\\u044C\\u0437\\u043E\\u0432\\u0430\\u0442\\u0435\\u043B\\u044C  \\xD7\\u041F\\u043E\\u0434\\u043F\\u0438\\u0441\\u0430\\u0442\\u044C\\u0441\\u044F \\u041A\\u043E\\u043C\\u043C\\u0435\\u043D\\u0442\\u0430\\u0440\\u0438\\u0438 1    vtikunov 23.02.2022 \\u0432 12:51 \\u0418\\u043D\\u0442\\u0435\\u0440\\u0435\\u0441\\u043D\\u043E, \\u0447\\u0442\\u043E \\u0437\\u0430 \\u043A\\u0435\\u0439\\u0441 \\u0442\\u0430\\u043A\\u043E\\u0439 \\u0441\\u043E 100\\u041C\\u0431 \\u0432 \\u043E\\u0434\\u043D\\u043E\\u043C \\u043F\\u043E\\u043B\\u0435. \\u041A\\u0430\\u0436\\u0435\\u0442\\u0441\\u044F \\u043B\\u043E\\u0433\\u0438\\u0447\\u043D\\u044B\\u043C, \\u0447\\u0442\\u043E postgres \\u0438\\u0437 \\u043A\\u043E\\u0440\\u043E\\u0431\\u043A\\u0438 \\u043D\\u0435 \\u043D\\u043E\\u0440\\u043C\\u0430\\u043B\\u044C\\u043D\\u043E \\u0440\\u0430\\u0431\\u043E\\u0442\\u0430\\u0435\\u0442 \\u0441 \\u0442\\u0430\\u043A\\u0438\\u043C \\u0440\\u0430\\u0437\\u043C\\u0435\\u0440\\u043E\\u043C.  0 \\u0412\\u044B \\u043C\\u043E\\u0436\\u0435\\u0442\\u0435 \\u043E\\u0441\\u0442\\u0430\\u0432\\u043B\\u044F\\u0442\\u044C \\u043A\\u043E\\u043C\\u043C\\u0435\\u043D\\u0442\\u0430\\u0440\\u0438\\u0438 \\u0442\\u043E\\u043B\\u044C\\u043A\\u043E \\u043A \\u0441\\u0432\\u0435\\u0436\\u0438\\u043C \\u043F\\u043E\\u0441\\u0442\\u0430\\u043C \\u041F\\u041E\\u0425\\u041E\\u0416\\u0418\\u0415 \\u041F\\u0423\\u0411\\u041B\\u0418\\u041A\\u0410\\u0426\\u0418\\u0418 28 \\u0430\\u043F\\u0440\\u0435\\u043B\\u044F \\u0432 13:49 \\u0421\\u0432\\u043E\\u0431\\u043E\\u0434\\u043D\\u044B\\u0439 \\u0434\\u043E\\u0441\\u0442\\u0443\\u043F \\u043A \\u0442\\u0440\\u0430\\u043D\\u0441\\u043B\\u044F\\u0446\\u0438\\u0438 \\u0438\\u0437 \\xAB\\u0422\\u0435\\u0440\\u043C\\u0438\\u043D\\u0443\\u0441\\u0430\\xBB, \\u0433\\u043B\\u0430\\u0432\\u043D\\u043E\\u0433\\u043E \\u0437\\u0430\\u043B\\u0430 Highload++ Foundation 2022 +7 513  3 0 24 \\u044F\\u043D\\u0432\\u0430\\u0440\\u044F \\u0432 00:26 \\u0411\\u043E\\u0440\\u044C\\u0431\\u0430 \\u0441  TOAST \\u0438\\u043B\\u0438 \\u0431\\u0443\\u0434\\u0443\\u0449\\u0435\\u0435 JSONB \\u0432 PostgreSQL +56 13K  71 8 27 \\u0434\\u0435\\u043A\\u0430\\u0431\\u0440\\u044F 2021 \\u0432 11:10 \\u041F\\u0440\\u043E\\u043A\\u043B\\u044F\\u0442\\u044C\\u0435 TOAST \\u0438 \\u0441 \\u043A\\u0430\\u043A\\u0438\\u043C \\u043C\\u0430\\u0441\\u043B\\u043E\\u043C \\u0435\\u0433\\u043E \\u0435\\u0441\\u0442 JSONB +25 6.9K  28 0 \\u041B\\u0423\\u0427\\u0428\\u0418\\u0415 \\u041F\\u0423\\u0411\\u041B\\u0418\\u041A\\u0410\\u0426\\u0418\\u0418 \\u0417\\u0410 \\u0421\\u0423\\u0422\\u041A\\u0418 \\u0441\\u0435\\u0433\\u043E\\u0434\\u043D\\u044F \\u0432 09:22 \\u0412\\u0442\\u043E\\u0440\\u0430\\u044F \\u0436\\u0438\\u0437\\u043D\\u044C \\u043A\\u0438\\u0442\\u0430\\u0439\\u0441\\u043A\\u043E\\u0433\\u043E \\u0447\\u0443\\u0434\\u043E-\\u0448\\u043D\\u0443\\u0440\\u043A\\u0430 J2534 +40 6.8K  15 10 +10\" #@param {type:\"string\"}\n",
        "stexts = texts\n",
        "print(texts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "То есть это  открывает большие перспективы. В PostgreSQL действительно вполне можно  стримить, если использовать наши оптимизации.  Здесь показано, как растет размер WAL. Справа он зависит только от размера добавляемого куска, а слева — от самой строки.   Мы посчитали скорость (Мб/с) для оригинального постгреса (слева) и с оптимизацией.  Слева мы ограничились случаем, когда мы добавляем  10-байтные кусочки, иначе всё будет слишком медленно. Мы видим, что скорость очень быстро падает с  1 Мб/с до 1 Кб/с, полоса совсем маленькая. Справа мы видим, что  производительность дописывания  не зависит от размера данных для любых размеров добавлений, и можно предсказать, что если мы хотим занять полосу 20 Мб/с, то надо апдейтить килобайтными чанками и деградации производительности не будет.   Вместо Заключения В этой серии статей я рассказал про возможности улучшения PostgreSQL для эффективного хранения больших значений на примере популярного типа данных JSONB и быстрого дописывания бинарных данных. PostgreSQL славится своей расширяемостью, поэтому логично ее расширить и на TOAST — так чтобы хранение больших значений было datatype aware.  Мы предложили серию патчей, реализующих API для TOAST (см. Pluggable TOAST), на основе которого можно разрабатывать TOAST, оптимизированный для определенного типа данных. Например, все описанные оптимизации для JSONB можно реализовать в виде расширения. Надеемся закоммитить все это для следующей версии PG15.  Видео моего выступления на Saint HighLoad++ 2021:   Конференция HighLoad++ Foundation 2022 пройдет 17 и 18 марта в Москве в Крокус Экспо. — 1 февраля. Планируйте свое участие, расписание и полный список тем с тезисами уже на сайте.  Наш доклад с коллегами — Pluggable TOAST or One TOAST fits ALL — будет логичным продолжением того, что я рассказал в этой серии статей. Билеты можно купить здесь.  До встречи на конференции!  Теги: postgresqlpostgresjsonjsonbstreamingstream processinghigh performancehighload++postgresql performancetoast Хабы: Блог компании Конференции Олега Бунина (Онтико)Высокая производительностьPostgreSQLАдминистрирование баз данныхХранилища данных +33  58 1   Конференции Олега Бунина (Онтико) Профессиональные конференции для IT-разработчиков ×Подписаться FacebookFacebookTwitterTelegram  66 Карма 0 Рейтинг Oleg Bartunov @zen Пользователь  ×Подписаться Комментарии 1    vtikunov 23.02.2022 в 12:51 Интересно, что за кейс такой со 100Мб в одном поле. Кажется логичным, что postgres из коробки не нормально работает с таким размером.  0 Вы можете оставлять комментарии только к свежим постам ПОХОЖИЕ ПУБЛИКАЦИИ 28 апреля в 13:49 Свободный доступ к трансляции из «Терминуса», главного зала Highload++ Foundation 2022 +7 513  3 0 24 января в 00:26 Борьба с  TOAST или будущее JSONB в PostgreSQL +56 13K  71 8 27 декабря 2021 в 11:10 Проклятье TOAST и с каким маслом его ест JSONB +25 6.9K  28 0 ЛУЧШИЕ ПУБЛИКАЦИИ ЗА СУТКИ сегодня в 09:22 Вторая жизнь китайского чудо-шнурка J2534 +40 6.8K  15 10 +10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFgHUfhAVu60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b26295b-31ac-423d-cf62-6916e7210cd5"
      },
      "source": [
        "# !pip install scikit-learn\n",
        "!pip install pandas==1.2.3\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas==1.2.3 in /usr/local/lib/python3.7/dist-packages (1.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.3) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.3) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.3) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.2.3) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuJXdzPXPXfF"
      },
      "source": [
        "def printTF_IDF(texts):\n",
        "  records_count = 30\n",
        "  tfIdfTransformer = TfidfVectorizer(ngram_range=(1, 4), use_idf=True, max_features=records_count)\n",
        "  countVectorizer = CountVectorizer(ngram_range=(1, 4), max_features=records_count)\n",
        "  wordCount = countVectorizer.fit_transform([texts])\n",
        "  TfIdf = tfIdfTransformer.fit_transform([texts])\n",
        "  names = countVectorizer.get_feature_names()\n",
        "  df=[]\n",
        "  df = pd.DataFrame(list(names), columns=['names'])\n",
        "  df = df.assign(Word_Count=wordCount.T.todense())\n",
        "  df = df.assign(TF_IDF=TfIdf.T.todense())\n",
        "  df = df.sort_values('TF_IDF', ascending=False)\n",
        "  print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuopkLIjPjkG",
        "outputId": "22a9b588-c26a-4f35-b48d-4cc27106b8f6"
      },
      "source": [
        "printTF_IDF(texts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  names  Word_Count    TF_IDF\n",
            "6                 toast           8  0.364013\n",
            "9                   для           8  0.364013\n",
            "16                   мы           7  0.318511\n",
            "29                  что           6  0.273009\n",
            "19                   на           6  0.273009\n",
            "8                данных           5  0.227508\n",
            "15                можно           5  0.227508\n",
            "5            postgresql           5  0.227508\n",
            "0                    10           4  0.182006\n",
            "4                 jsonb           4  0.182006\n",
            "13          конференции           4  0.182006\n",
            "3              highload           3  0.136505\n",
            "28                слева           3  0.136505\n",
            "25                   от           3  0.136505\n",
            "20                   не           3  0.136505\n",
            "7                 будет           3  0.136505\n",
            "1                  2022           3  0.136505\n",
            "14                   мб           3  0.136505\n",
            "17             мы видим           2  0.091003\n",
            "2                    28           2  0.091003\n",
            "12                   из           2  0.091003\n",
            "21                олега           2  0.091003\n",
            "22         олега бунина           2  0.091003\n",
            "23  олега бунина онтико           2  0.091003\n",
            "24          оптимизации           2  0.091003\n",
            "11          дописывания           2  0.091003\n",
            "26          подписаться           2  0.091003\n",
            "27            рассказал           2  0.091003\n",
            "10                   до           2  0.091003\n",
            "18         мы видим что           2  0.091003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_rQ7bdtxilr"
      },
      "source": [
        "удалим знаки переноса строк и сделаем текст в нижнем регистре"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIEOKhJGxeKr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a700eea-c76b-4a8f-f88f-86d8d9e28977"
      },
      "source": [
        "  str2 = ''\n",
        "  for item in texts.split():\n",
        "    str2 = str2 + ' ' + item\n",
        "  texts = str(texts)\n",
        "  texts = texts.lower()\n",
        "  # print(texts)\n",
        "  texts = texts.replace('\\n', ' ')\n",
        "  print(texts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "то есть это  открывает большие перспективы. в postgresql действительно вполне можно  стримить, если использовать наши оптимизации.  здесь показано, как растет размер wal. справа он зависит только от размера добавляемого куска, а слева — от самой строки.   мы посчитали скорость (мб/с) для оригинального постгреса (слева) и с оптимизацией.  слева мы ограничились случаем, когда мы добавляем  10-байтные кусочки, иначе всё будет слишком медленно. мы видим, что скорость очень быстро падает с  1 мб/с до 1 кб/с, полоса совсем маленькая. справа мы видим, что  производительность дописывания  не зависит от размера данных для любых размеров добавлений, и можно предсказать, что если мы хотим занять полосу 20 мб/с, то надо апдейтить килобайтными чанками и деградации производительности не будет.   вместо заключения в этой серии статей я рассказал про возможности улучшения postgresql для эффективного хранения больших значений на примере популярного типа данных jsonb и быстрого дописывания бинарных данных. postgresql славится своей расширяемостью, поэтому логично ее расширить и на toast — так чтобы хранение больших значений было datatype aware.  мы предложили серию патчей, реализующих api для toast (см. pluggable toast), на основе которого можно разрабатывать toast, оптимизированный для определенного типа данных. например, все описанные оптимизации для jsonb можно реализовать в виде расширения. надеемся закоммитить все это для следующей версии pg15.  видео моего выступления на saint highload++ 2021:   конференция highload++ foundation 2022 пройдет 17 и 18 марта в москве в крокус экспо. — 1 февраля. планируйте свое участие, расписание и полный список тем с тезисами уже на сайте.  наш доклад с коллегами — pluggable toast or one toast fits all — будет логичным продолжением того, что я рассказал в этой серии статей. билеты можно купить здесь.  до встречи на конференции!  теги: postgresqlpostgresjsonjsonbstreamingstream processinghigh performancehighload++postgresql performancetoast хабы: блог компании конференции олега бунина (онтико)высокая производительностьpostgresqlадминистрирование баз данныххранилища данных +33  58 1   конференции олега бунина (онтико) профессиональные конференции для it-разработчиков ×подписаться facebookfacebooktwittertelegram  66 карма 0 рейтинг oleg bartunov @zen пользователь  ×подписаться комментарии 1    vtikunov 23.02.2022 в 12:51 интересно, что за кейс такой со 100мб в одном поле. кажется логичным, что postgres из коробки не нормально работает с таким размером.  0 вы можете оставлять комментарии только к свежим постам похожие публикации 28 апреля в 13:49 свободный доступ к трансляции из «терминуса», главного зала highload++ foundation 2022 +7 513  3 0 24 января в 00:26 борьба с  toast или будущее jsonb в postgresql +56 13k  71 8 27 декабря 2021 в 11:10 проклятье toast и с каким маслом его ест jsonb +25 6.9k  28 0 лучшие публикации за сутки сегодня в 09:22 вторая жизнь китайского чудо-шнурка j2534 +40 6.8k  15 10 +10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0noHUfJNxy5x"
      },
      "source": [
        "удалим цифры"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnKupWBnxx80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cc1b995-f342-409f-b944-dea0705031b2"
      },
      "source": [
        "  str2 = ''\n",
        "  for c in texts:\n",
        "      if c not in ('0', \"1\", '2', '3', '4', '5', '6', '7', '8', '9', '«', '»', '–', \"\\\"\"):\n",
        "          str2 = str2 + c\n",
        "  texts = str2\n",
        "  str2 = ''\n",
        "  print(texts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "то есть это  открывает большие перспективы. в postgresql действительно вполне можно  стримить, если использовать наши оптимизации.  здесь показано, как растет размер wal. справа он зависит только от размера добавляемого куска, а слева — от самой строки.   мы посчитали скорость (мб/с) для оригинального постгреса (слева) и с оптимизацией.  слева мы ограничились случаем, когда мы добавляем  -байтные кусочки, иначе всё будет слишком медленно. мы видим, что скорость очень быстро падает с   мб/с до  кб/с, полоса совсем маленькая. справа мы видим, что  производительность дописывания  не зависит от размера данных для любых размеров добавлений, и можно предсказать, что если мы хотим занять полосу  мб/с, то надо апдейтить килобайтными чанками и деградации производительности не будет.   вместо заключения в этой серии статей я рассказал про возможности улучшения postgresql для эффективного хранения больших значений на примере популярного типа данных jsonb и быстрого дописывания бинарных данных. postgresql славится своей расширяемостью, поэтому логично ее расширить и на toast — так чтобы хранение больших значений было datatype aware.  мы предложили серию патчей, реализующих api для toast (см. pluggable toast), на основе которого можно разрабатывать toast, оптимизированный для определенного типа данных. например, все описанные оптимизации для jsonb можно реализовать в виде расширения. надеемся закоммитить все это для следующей версии pg.  видео моего выступления на saint highload++ :   конференция highload++ foundation  пройдет  и  марта в москве в крокус экспо. —  февраля. планируйте свое участие, расписание и полный список тем с тезисами уже на сайте.  наш доклад с коллегами — pluggable toast or one toast fits all — будет логичным продолжением того, что я рассказал в этой серии статей. билеты можно купить здесь.  до встречи на конференции!  теги: postgresqlpostgresjsonjsonbstreamingstream processinghigh performancehighload++postgresql performancetoast хабы: блог компании конференции олега бунина (онтико)высокая производительностьpostgresqlадминистрирование баз данныххранилища данных +      конференции олега бунина (онтико) профессиональные конференции для it-разработчиков ×подписаться facebookfacebooktwittertelegram   карма  рейтинг oleg bartunov @zen пользователь  ×подписаться комментарии     vtikunov .. в : интересно, что за кейс такой со мб в одном поле. кажется логичным, что postgres из коробки не нормально работает с таким размером.   вы можете оставлять комментарии только к свежим постам похожие публикации  апреля в : свободный доступ к трансляции из терминуса, главного зала highload++ foundation  +      января в : борьба с  toast или будущее jsonb в postgresql + k     декабря  в : проклятье toast и с каким маслом его ест jsonb + .k    лучшие публикации за сутки сегодня в : вторая жизнь китайского чудо-шнурка j + .k    +\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99rmNX2Mx6OU"
      },
      "source": [
        "удалим знаки пунктуации"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWd0WHq7x5mI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c6b13a8-fc0e-463f-9cc3-83c04b0b80cf"
      },
      "source": [
        "import string\n",
        "pattern = string.punctuation\n",
        "for c in texts:\n",
        "    if c not in pattern:\n",
        "        str2 = str2 + c\n",
        "    else:\n",
        "        str2 = str2 + \" \"\n",
        "texts = str2\n",
        "str2 = ''\n",
        "print(texts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "то есть это  открывает большие перспективы  в postgresql действительно вполне можно  стримить  если использовать наши оптимизации   здесь показано  как растет размер wal  справа он зависит только от размера добавляемого куска  а слева — от самой строки    мы посчитали скорость  мб с  для оригинального постгреса  слева  и с оптимизацией   слева мы ограничились случаем  когда мы добавляем   байтные кусочки  иначе всё будет слишком медленно  мы видим  что скорость очень быстро падает с   мб с до  кб с  полоса совсем маленькая  справа мы видим  что  производительность дописывания  не зависит от размера данных для любых размеров добавлений  и можно предсказать  что если мы хотим занять полосу  мб с  то надо апдейтить килобайтными чанками и деградации производительности не будет    вместо заключения в этой серии статей я рассказал про возможности улучшения postgresql для эффективного хранения больших значений на примере популярного типа данных jsonb и быстрого дописывания бинарных данных  postgresql славится своей расширяемостью  поэтому логично ее расширить и на toast — так чтобы хранение больших значений было datatype aware   мы предложили серию патчей  реализующих api для toast  см  pluggable toast   на основе которого можно разрабатывать toast  оптимизированный для определенного типа данных  например  все описанные оптимизации для jsonb можно реализовать в виде расширения  надеемся закоммитить все это для следующей версии pg   видео моего выступления на saint highload       конференция highload   foundation  пройдет  и  марта в москве в крокус экспо  —  февраля  планируйте свое участие  расписание и полный список тем с тезисами уже на сайте   наш доклад с коллегами — pluggable toast or one toast fits all — будет логичным продолжением того  что я рассказал в этой серии статей  билеты можно купить здесь   до встречи на конференции   теги  postgresqlpostgresjsonjsonbstreamingstream processinghigh performancehighload  postgresql performancetoast хабы  блог компании конференции олега бунина  онтико высокая производительностьpostgresqlадминистрирование баз данныххранилища данных        конференции олега бунина  онтико  профессиональные конференции для it разработчиков ×подписаться facebookfacebooktwittertelegram   карма  рейтинг oleg bartunov  zen пользователь  ×подписаться комментарии     vtikunov    в   интересно  что за кейс такой со мб в одном поле  кажется логичным  что postgres из коробки не нормально работает с таким размером    вы можете оставлять комментарии только к свежим постам похожие публикации  апреля в   свободный доступ к трансляции из терминуса  главного зала highload   foundation         января в   борьба с  toast или будущее jsonb в postgresql   k     декабря  в   проклятье toast и с каким маслом его ест jsonb    k    лучшие публикации за сутки сегодня в   вторая жизнь китайского чудо шнурка j    k     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKuMC8gLyI9s"
      },
      "source": [
        "загрузим стоп слова, используем библиотеку NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Prm51-xVQv0W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed0182de-92ee-4895-b9e4-19e1c4a4c8fc"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Добавить свои стоп-слова:"
      ],
      "metadata": {
        "id": "5E_8F8222gdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_stop_words = \"\\u0420\\u0443\\u0441\\u0430\\u043A\\u043E\\u0432 \\u041C\\u0438\\u0445\\u0430\\u043B\\u044B\\u0447\" #@param {type:\"string\"}\n",
        "print(my_stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYIAVC6a2oh1",
        "outputId": "003a56fe-8440-42d9-a82a-b791baba4ca0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Русаков Михалыч\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3g8EhC2h4pJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Удалим стоп слова из текста"
      ],
      "metadata": {
        "id": "4uQk--w84C7N"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2UZ3uiOXReP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3a6b6bf-01f5-4bae-9d26-3c9e78bb21ec"
      },
      "source": [
        "  from nltk.corpus import stopwords\n",
        "  russian_stopwords = stopwords.words(\"russian\")\n",
        "\n",
        "  for word in my_stop_words.split():\n",
        "    russian_stopwords.append(word)\n",
        "  str2 = ''\n",
        "  for word in texts.split():\n",
        "      if word not in (russian_stopwords):\n",
        "          str2 = str2 + \" \" + word\n",
        "  texts = str2\n",
        "  str2 = ''\n",
        "  for word in texts.split():\n",
        "      if len(word) > 1:\n",
        "          str2 = str2 + \" \" + word\n",
        "  texts = str2\n",
        "  print(russian_stopwords)\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между', 'Русаков', 'Михалыч']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxdYblpjyLkO"
      },
      "source": [
        "рассчитаем TF-IDF для отфильтрованных данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoQJ1ALGq3yf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6376cbc-fc2c-4352-c156-94eca8e26c8d"
      },
      "source": [
        "printTF_IDF(texts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                              names  Word_Count    TF_IDF\n",
            "6                             toast           8  0.488678\n",
            "9                            данных           5  0.305424\n",
            "5                        postgresql           5  0.305424\n",
            "19                               мб           4  0.244339\n",
            "2                             jsonb           4  0.244339\n",
            "14                      конференции           4  0.244339\n",
            "0                          highload           3  0.183254\n",
            "25                            слева           3  0.183254\n",
            "26                           справа           2  0.122169\n",
            "18                         логичным           2  0.122169\n",
            "27                           статей           2  0.122169\n",
            "24                      оптимизации           2  0.122169\n",
            "23                           онтико           2  0.122169\n",
            "22              олега бунина онтико           2  0.122169\n",
            "21                     олега бунина           2  0.122169\n",
            "28                             типа           2  0.122169\n",
            "20                            олега           2  0.122169\n",
            "15                конференции олега           2  0.122169\n",
            "17  конференции олега бунина онтико           2  0.122169\n",
            "16         конференции олега бунина           2  0.122169\n",
            "1               highload foundation           2  0.122169\n",
            "13                      комментарии           2  0.122169\n",
            "12                  зависит размера           2  0.122169\n",
            "11                          зависит           2  0.122169\n",
            "10                      дописывания           2  0.122169\n",
            "8                            бунина           2  0.122169\n",
            "7                           больших           2  0.122169\n",
            "4                   pluggable toast           2  0.122169\n",
            "3                         pluggable           2  0.122169\n",
            "29                      типа данных           2  0.122169\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Лемматизация\n",
        "Лемматизация (англ. lemmatization) - это метод морфологического анализа, который сводится к приведению словоформы к ее первоначальной словарной форме (лемме).\n",
        "\n",
        "Метод лемматизации применяется в поисковых алгоритмах в процессе схематизации веб-документов при их индексировании.\n",
        "\n",
        "В результате лемматизации от словоформы отбрасываются флективные окончания и возвращается основная или словарная форма слова.\n",
        "\n",
        "Например, в русском языке словарной формой считается:\n",
        "\n",
        "существительные - именительный падеж, единственное число (мечами - меч)\n",
        "глаголы - инфинитивная форма (читали - читать)\n",
        "прилагательные - единственное число, именительный падеж, мужской род (заснеженными - заснеженный)"
      ],
      "metadata": {
        "id": "tvOVaBe1_dYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy2\n",
        "import pymorphy2\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "ltexts = \"\"\n",
        "print(texts)\n",
        "for word in texts.split():\n",
        "    if len(str(word)) > 2:\n",
        "        ltexts+=\" \" + morph.parse(word)[0].normal_form\n",
        "print(ltexts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-blA95F_rNd",
        "outputId": "a904e769-eee3-427b-aae0-e5326e02aa45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.7/dist-packages (0.9.1)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.7.2)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (2.4.417127.4579844)\n",
            " это открывает большие перспективы postgresql действительно вполне стримить использовать наши оптимизации показано растет размер wal справа зависит размера добавляемого куска слева самой строки посчитали скорость мб оригинального постгреса слева оптимизацией слева ограничились случаем добавляем байтные кусочки иначе всё слишком медленно видим скорость очень быстро падает мб кб полоса маленькая справа видим производительность дописывания зависит размера данных любых размеров добавлений предсказать хотим занять полосу мб апдейтить килобайтными чанками деградации производительности вместо заключения серии статей рассказал возможности улучшения postgresql эффективного хранения больших значений примере популярного типа данных jsonb быстрого дописывания бинарных данных postgresql славится своей расширяемостью поэтому логично расширить toast хранение больших значений datatype aware предложили серию патчей реализующих api toast см pluggable toast основе которого разрабатывать toast оптимизированный определенного типа данных например описанные оптимизации jsonb реализовать виде расширения надеемся закоммитить это следующей версии pg видео моего выступления saint highload конференция highload foundation пройдет марта москве крокус экспо февраля планируйте свое участие расписание полный список тезисами сайте наш доклад коллегами pluggable toast or one toast fits all логичным продолжением рассказал серии статей билеты купить встречи конференции теги postgresqlpostgresjsonjsonbstreamingstream processinghigh performancehighload postgresql performancetoast хабы блог компании конференции олега бунина онтико высокая производительностьpostgresqlадминистрирование баз данныххранилища данных конференции олега бунина онтико профессиональные конференции it разработчиков ×подписаться facebookfacebooktwittertelegram карма рейтинг oleg bartunov zen пользователь ×подписаться комментарии vtikunov интересно кейс мб одном поле кажется логичным postgres коробки нормально работает таким размером можете оставлять комментарии свежим постам похожие публикации апреля свободный доступ трансляции терминуса главного зала highload foundation января борьба toast будущее jsonb postgresql декабря проклятье toast каким маслом ест jsonb лучшие публикации сутки сегодня вторая жизнь китайского чудо шнурка\n",
            " это открывать больший перспектива postgresql действительно вполне стримитя использовать наш оптимизация показать расти размер wal справа зависеть размер добавлять кусок слева сам строка посчитать скорость оригинальный постгрес слева оптимизация слева ограничиться случаем добавлять байтный кусочек иначе всё слишком медленно видеть скорость очень быстро падать полоса маленький справа видеть производительность дописывание зависеть размер данные любой размер добавление предсказать хотеть занять полоса апдейтить килобайтный чанк деградация производительность вместо заключение серия стать рассказать возможность улучшение postgresql эффективный хранение больший значение пример популярный тип данные jsonb быстрый дописывание бинарный данные postgresql славиться свой расширяемость поэтому логично расширить toast хранение больший значение datatype aware предложить серия патч реализовать api toast pluggable toast основа который разрабатывать toast оптимизировать определённый тип данные например описать оптимизация jsonb реализовать вид расширение надеяться закоммитить это следующий версия видео мой выступление saint highload конференция highload foundation пройти март москва крокус экспо февраль планировать свой участие расписание полный список тезис сайт наш доклад коллега pluggable toast one toast fits all логичный продолжение рассказать серия стать билет купить встреча конференция тег postgresqlpostgresjsonjsonbstreamingstream processinghigh performancehighload postgresql performancetoast хаба блог компания конференция олег бунин онтико высокий производительностьpostgresqlадминистрирование база данныххранилище данные конференция олег бунин онтико профессиональный конференция разработчик ×подписаться facebookfacebooktwittertelegram карма рейтинг oleg bartunov zen пользователь ×подписаться комментарий vtikunov интересно кейс один поле казаться логичный postgres коробка нормально работать такой размер мочь оставлять комментарий свежий пост похожий публикация апрель свободный доступ трансляция терминус главное зал highload foundation январь борьба toast будущее jsonb postgresql декабрь проклятие toast какой масло есть jsonb хороший публикация сутки сегодня второй жизнь китайский чудо шнурок\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pymorphy2/units/base.py:70: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  args, varargs, kw, default = inspect.getargspec(cls.__init__)\n",
            "/usr/local/lib/python3.7/dist-packages/pymorphy2/units/base.py:70: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  args, varargs, kw, default = inspect.getargspec(cls.__init__)\n",
            "/usr/local/lib/python3.7/dist-packages/pymorphy2/units/base.py:70: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  args, varargs, kw, default = inspect.getargspec(cls.__init__)\n",
            "/usr/local/lib/python3.7/dist-packages/pymorphy2/units/base.py:70: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  args, varargs, kw, default = inspect.getargspec(cls.__init__)\n",
            "/usr/local/lib/python3.7/dist-packages/pymorphy2/units/base.py:70: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  args, varargs, kw, default = inspect.getargspec(cls.__init__)\n",
            "/usr/local/lib/python3.7/dist-packages/pymorphy2/units/base.py:70: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  args, varargs, kw, default = inspect.getargspec(cls.__init__)\n",
            "/usr/local/lib/python3.7/dist-packages/pymorphy2/units/base.py:70: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  args, varargs, kw, default = inspect.getargspec(cls.__init__)\n",
            "/usr/local/lib/python3.7/dist-packages/pymorphy2/units/base.py:70: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  args, varargs, kw, default = inspect.getargspec(cls.__init__)\n",
            "/usr/local/lib/python3.7/dist-packages/pymorphy2/units/base.py:70: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  args, varargs, kw, default = inspect.getargspec(cls.__init__)\n",
            "/usr/local/lib/python3.7/dist-packages/pymorphy2/units/base.py:70: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  args, varargs, kw, default = inspect.getargspec(cls.__init__)\n",
            "/usr/local/lib/python3.7/dist-packages/pymorphy2/units/base.py:70: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  args, varargs, kw, default = inspect.getargspec(cls.__init__)\n",
            "/usr/local/lib/python3.7/dist-packages/pymorphy2/units/base.py:70: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  args, varargs, kw, default = inspect.getargspec(cls.__init__)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "рассчитаем TF-IDF для Лемматизированных данных"
      ],
      "metadata": {
        "id": "5nCg3sFP_2ly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "printTF_IDF(ltexts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwoJJWxe__p_",
        "outputId": "11a1246b-a45b-41a7-9ee1-c9fd067cedc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  names  Word_Count    TF_IDF\n",
            "4                 toast           8  0.461112\n",
            "19               размер           5  0.288195\n",
            "3            postgresql           5  0.288195\n",
            "7                данные           5  0.288195\n",
            "11          конференция           5  0.288195\n",
            "2                 jsonb           4  0.230556\n",
            "0              highload           3  0.172917\n",
            "23                серия           3  0.172917\n",
            "24                слева           3  0.172917\n",
            "17          оптимизация           3  0.172917\n",
            "5               больший           3  0.172917\n",
            "25               справа           2  0.115278\n",
            "27                  тип           2  0.115278\n",
            "28           тип данные           2  0.115278\n",
            "22                 свой           2  0.115278\n",
            "21          реализовать           2  0.115278\n",
            "20           рассказать           2  0.115278\n",
            "26                стать           2  0.115278\n",
            "15    олег бунин онтико           2  0.115278\n",
            "18   производительность           2  0.115278\n",
            "16               онтико           2  0.115278\n",
            "1   highload foundation           2  0.115278\n",
            "14           олег бунин           2  0.115278\n",
            "13                 олег           2  0.115278\n",
            "12             логичный           2  0.115278\n",
            "10      зависеть размер           2  0.115278\n",
            "9           дописывание           2  0.115278\n",
            "8             добавлять           2  0.115278\n",
            "6                видеть           2  0.115278\n",
            "29             хранение           2  0.115278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wy3zm9eTvla"
      },
      "source": [
        "#Суммаризации текстов \n",
        "\n",
        "Задача суммаризации текстов (автореферирование) - одна из ключевых, широко обсуждаемых задач NLP. Она состоит в сжатии больших объемов текста до связного краткого содержания, отражающего только основные идеи.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrZmQkMpRBPJ"
      },
      "source": [
        "#Алгоритм TextRank\n",
        "TextRank - это алгоритм, основанный на PageRank, который часто используется для извлечения ключевых слов и суммирования текста. \n",
        "PageRank (PR) - это алгоритм, используемый для расчета веса веб-страниц. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiBF_PrZUbzG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8a6161e-f7c5-4268-f88b-fa87611e3087"
      },
      "source": [
        "from gensim.summarization import summarize, keywords\n",
        "print(\"Исходный текст:\")\n",
        "print(stexts)\n",
        "print(\"\\nРезультат работы TextRank:\")\n",
        "print(summarize(str(stexts)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Исходный текст:\n",
            "То есть это  открывает большие перспективы. В PostgreSQL действительно вполне можно  стримить, если использовать наши оптимизации.  Здесь показано, как растет размер WAL. Справа он зависит только от размера добавляемого куска, а слева — от самой строки.   Мы посчитали скорость (Мб/с) для оригинального постгреса (слева) и с оптимизацией.  Слева мы ограничились случаем, когда мы добавляем  10-байтные кусочки, иначе всё будет слишком медленно. Мы видим, что скорость очень быстро падает с  1 Мб/с до 1 Кб/с, полоса совсем маленькая. Справа мы видим, что  производительность дописывания  не зависит от размера данных для любых размеров добавлений, и можно предсказать, что если мы хотим занять полосу 20 Мб/с, то надо апдейтить килобайтными чанками и деградации производительности не будет.   Вместо Заключения В этой серии статей я рассказал про возможности улучшения PostgreSQL для эффективного хранения больших значений на примере популярного типа данных JSONB и быстрого дописывания бинарных данных. PostgreSQL славится своей расширяемостью, поэтому логично ее расширить и на TOAST — так чтобы хранение больших значений было datatype aware.  Мы предложили серию патчей, реализующих API для TOAST (см. Pluggable TOAST), на основе которого можно разрабатывать TOAST, оптимизированный для определенного типа данных. Например, все описанные оптимизации для JSONB можно реализовать в виде расширения. Надеемся закоммитить все это для следующей версии PG15.  Видео моего выступления на Saint HighLoad++ 2021:   Конференция HighLoad++ Foundation 2022 пройдет 17 и 18 марта в Москве в Крокус Экспо. — 1 февраля. Планируйте свое участие, расписание и полный список тем с тезисами уже на сайте.  Наш доклад с коллегами — Pluggable TOAST or One TOAST fits ALL — будет логичным продолжением того, что я рассказал в этой серии статей. Билеты можно купить здесь.  До встречи на конференции!  Теги: postgresqlpostgresjsonjsonbstreamingstream processinghigh performancehighload++postgresql performancetoast Хабы: Блог компании Конференции Олега Бунина (Онтико)Высокая производительностьPostgreSQLАдминистрирование баз данныхХранилища данных +33  58 1   Конференции Олега Бунина (Онтико) Профессиональные конференции для IT-разработчиков ×Подписаться FacebookFacebookTwitterTelegram  66 Карма 0 Рейтинг Oleg Bartunov @zen Пользователь  ×Подписаться Комментарии 1    vtikunov 23.02.2022 в 12:51 Интересно, что за кейс такой со 100Мб в одном поле. Кажется логичным, что postgres из коробки не нормально работает с таким размером.  0 Вы можете оставлять комментарии только к свежим постам ПОХОЖИЕ ПУБЛИКАЦИИ 28 апреля в 13:49 Свободный доступ к трансляции из «Терминуса», главного зала Highload++ Foundation 2022 +7 513  3 0 24 января в 00:26 Борьба с  TOAST или будущее JSONB в PostgreSQL +56 13K  71 8 27 декабря 2021 в 11:10 Проклятье TOAST и с каким маслом его ест JSONB +25 6.9K  28 0 ЛУЧШИЕ ПУБЛИКАЦИИ ЗА СУТКИ сегодня в 09:22 Вторая жизнь китайского чудо-шнурка J2534 +40 6.8K  15 10 +10\n",
            "\n",
            "Результат работы TextRank:\n",
            "Справа мы видим, что  производительность дописывания  не зависит от размера данных для любых размеров добавлений, и можно предсказать, что если мы хотим занять полосу 20 Мб/с, то надо апдейтить килобайтными чанками и деградации производительности не будет.\n",
            "Вместо Заключения В этой серии статей я рассказал про возможности улучшения PostgreSQL для эффективного хранения больших значений на примере популярного типа данных JSONB и быстрого дописывания бинарных данных.\n",
            "Pluggable TOAST), на основе которого можно разрабатывать TOAST, оптимизированный для определенного типа данных.\n",
            "Наш доклад с коллегами — Pluggable TOAST or One TOAST fits ALL — будет логичным продолжением того, что я рассказал в этой серии статей.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2hy7d1VGVm0"
      },
      "source": [
        "#Алгоритм Rake\n",
        "RAKE: Rapid Automatic Keyword Extraction Algorithm\n",
        "Алгоритм RAKE извлекает ключевые слова с помощью основанного на разделителе подхода, чтобы идентифицировать ключевые слова кандидата и баллы их использующий совместные встречаемости слова, которые появляются в ключевых словах кандидата. Ключевые слова могут содержать несколько лексем. Кроме того, алгоритм RAKE также объединяет ключевые слова, когда они кажутся многократно, разделенными тем же разделителем слияния.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdPdJagoH0Kb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a67a09f-b02a-4b67-de81-367de1b7aa8e"
      },
      "source": [
        "!pip install rake_nltk\n",
        "from rake_nltk import Metric, Rake\n",
        "r = Rake(language=\"russian\")\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "r.extract_keywords_from_text(stexts)\n",
        "mas = r.get_ranked_phrases()\n",
        "set2 = set()\n",
        "for item in mas:\n",
        "    if not \"nan\" in str(item).replace(\" nan \", \" \"):\n",
        "        set2.add(str(item).replace(\" nan \", \" \"))\n",
        "mas = list(set2)\n",
        "print(\"Исходный текст:\")\n",
        "print(stexts)\n",
        "print(\"\\nРезультат работы TextRammk:\")\n",
        "for item in mas:\n",
        "  print(item)\n",
        "# print(str(mas))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rake_nltk in /usr/local/lib/python3.7/dist-packages (1.0.6)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.6.2 in /usr/local/lib/python3.7/dist-packages (from rake_nltk) (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (4.64.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (2022.6.2)\n",
            "Исходный текст:\n",
            "То есть это  открывает большие перспективы. В PostgreSQL действительно вполне можно  стримить, если использовать наши оптимизации.  Здесь показано, как растет размер WAL. Справа он зависит только от размера добавляемого куска, а слева — от самой строки.   Мы посчитали скорость (Мб/с) для оригинального постгреса (слева) и с оптимизацией.  Слева мы ограничились случаем, когда мы добавляем  10-байтные кусочки, иначе всё будет слишком медленно. Мы видим, что скорость очень быстро падает с  1 Мб/с до 1 Кб/с, полоса совсем маленькая. Справа мы видим, что  производительность дописывания  не зависит от размера данных для любых размеров добавлений, и можно предсказать, что если мы хотим занять полосу 20 Мб/с, то надо апдейтить килобайтными чанками и деградации производительности не будет.   Вместо Заключения В этой серии статей я рассказал про возможности улучшения PostgreSQL для эффективного хранения больших значений на примере популярного типа данных JSONB и быстрого дописывания бинарных данных. PostgreSQL славится своей расширяемостью, поэтому логично ее расширить и на TOAST — так чтобы хранение больших значений было datatype aware.  Мы предложили серию патчей, реализующих API для TOAST (см. Pluggable TOAST), на основе которого можно разрабатывать TOAST, оптимизированный для определенного типа данных. Например, все описанные оптимизации для JSONB можно реализовать в виде расширения. Надеемся закоммитить все это для следующей версии PG15.  Видео моего выступления на Saint HighLoad++ 2021:   Конференция HighLoad++ Foundation 2022 пройдет 17 и 18 марта в Москве в Крокус Экспо. — 1 февраля. Планируйте свое участие, расписание и полный список тем с тезисами уже на сайте.  Наш доклад с коллегами — Pluggable TOAST or One TOAST fits ALL — будет логичным продолжением того, что я рассказал в этой серии статей. Билеты можно купить здесь.  До встречи на конференции!  Теги: postgresqlpostgresjsonjsonbstreamingstream processinghigh performancehighload++postgresql performancetoast Хабы: Блог компании Конференции Олега Бунина (Онтико)Высокая производительностьPostgreSQLАдминистрирование баз данныхХранилища данных +33  58 1   Конференции Олега Бунина (Онтико) Профессиональные конференции для IT-разработчиков ×Подписаться FacebookFacebookTwitterTelegram  66 Карма 0 Рейтинг Oleg Bartunov @zen Пользователь  ×Подписаться Комментарии 1    vtikunov 23.02.2022 в 12:51 Интересно, что за кейс такой со 100Мб в одном поле. Кажется логичным, что postgres из коробки не нормально работает с таким размером.  0 Вы можете оставлять комментарии только к свежим постам ПОХОЖИЕ ПУБЛИКАЦИИ 28 апреля в 13:49 Свободный доступ к трансляции из «Терминуса», главного зала Highload++ Foundation 2022 +7 513  3 0 24 января в 00:26 Борьба с  TOAST или будущее JSONB в PostgreSQL +56 13K  71 8 27 декабря 2021 в 11:10 Проклятье TOAST и с каким маслом его ест JSONB +25 6.9K  28 0 ЛУЧШИЕ ПУБЛИКАЦИИ ЗА СУТКИ сегодня в 09:22 Вторая жизнь китайского чудо-шнурка J2534 +40 6.8K  15 10 +10\n",
            "\n",
            "Результат работы TextRammk:\n",
            "поэтому логично\n",
            "100мб\n",
            "postgres\n",
            "это\n",
            "коробки\n",
            "показано\n",
            "56 13k 71 8 27 декабря 2021\n",
            "datatype aware\n",
            "профессиональные конференции\n",
            "каким маслом\n",
            "можете оставлять комментарии\n",
            "эффективного хранения больших значений\n",
            "купить\n",
            "1 мб\n",
            "сутки сегодня\n",
            "11\n",
            "0\n",
            "блог компании конференции олега бунина\n",
            "одном поле\n",
            "toast\n",
            "использовать наши оптимизации\n",
            "оригинального постгреса\n",
            "размера данных\n",
            "иначе всё\n",
            "слишком медленно\n",
            "это открывает большие перспективы\n",
            "зависит\n",
            "хотим занять полосу 20 мб\n",
            "51 интересно\n",
            "расширить\n",
            "теги\n",
            "слева —\n",
            "наш доклад\n",
            "1 кб\n",
            "2022\n",
            "toast —\n",
            "апдейтить килобайтными чанками\n",
            "виде расширения\n",
            "логичным продолжением\n",
            "it\n",
            "деградации производительности\n",
            "оптимизацией\n",
            "7 513 3 0 24 января\n",
            "билеты\n",
            "таким размером\n",
            "« терминуса », главного зала highload ++ foundation 2022\n",
            "saint highload ++ 2021\n",
            "встречи\n",
            "высокая производительностьpostgresqlадминистрирование баз данныххранилища данных\n",
            "09\n",
            "растет размер wal\n",
            "любых размеров добавлений\n",
            "планируйте свое участие\n",
            "zen пользователь × подписаться комментарии 1 vtikunov 23\n",
            "49 свободный доступ\n",
            "ограничились случаем\n",
            "pluggable toast ),\n",
            "байтные кусочки\n",
            "конференции\n",
            "стримить\n",
            "18 марта\n",
            "см\n",
            "jsonb\n",
            "26 борьба\n",
            "тезисами\n",
            "добавляем 10\n",
            "25 6\n",
            "мб\n",
            "посчитали скорость\n",
            "postgresqlpostgresjsonjsonbstreamingstream processinghigh performancehighload ++ postgresql performancetoast хабы\n",
            "postgresql действительно вполне\n",
            "трансляции\n",
            "описанные оптимизации\n",
            "будущее jsonb\n",
            "сайте\n",
            "примере популярного типа данных jsonb\n",
            "13\n",
            "самой строки\n",
            "ест jsonb\n",
            "40 6\n",
            "33 58 1 конференции олега бунина\n",
            "реализовать\n",
            "10 проклятье toast\n",
            "полоса\n",
            "свежим постам похожие публикации 28 апреля\n",
            "22 вторая жизнь китайского чудо\n",
            "серии статей\n",
            "маленькая\n",
            "надеемся закоммитить\n",
            "видим\n",
            "рассказал\n",
            "реализующих api\n",
            "postgresql\n",
            "8k 15 10\n",
            "видео моего выступления\n",
            "кажется логичным\n",
            "справа\n",
            "расписание\n",
            "онтико\n",
            "кейс\n",
            "производительность дописывания\n",
            "следующей версии pg15\n",
            "быстрого дописывания бинарных данных\n",
            "например\n",
            "— 1 февраля\n",
            "хранение больших значений\n",
            "разработчиков × подписаться facebookfacebooktwittertelegram 66 карма 0 рейтинг oleg bartunov\n",
            "разрабатывать toast\n",
            "предложили серию патчей\n",
            "вместо заключения\n",
            "москве\n",
            "postgresql славится своей расширяемостью\n",
            "12\n",
            "02\n",
            "00\n",
            "коллегами — pluggable toast or one toast fits all —\n",
            "слева\n",
            "конференция highload ++ foundation 2022 пройдет 17\n",
            "10\n",
            "предсказать\n",
            "определенного типа данных\n",
            "оптимизированный\n",
            "9k 28 0 лучшие публикации\n",
            "размера добавляемого куска\n",
            "крокус экспо\n",
            "нормально работает\n",
            "основе которого\n",
            "полный список\n",
            "скорость очень быстро падает\n",
            "возможности улучшения postgresql\n",
            "шнурка j2534\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49ShKaewJbds"
      },
      "source": [
        "#Тематическое моделирование\n",
        "Тематическое моделирование — это метод извлечения тем из текста. Latent Dirichlet Allocation (LDA) — популярный алгоритм моделирования тем реализованные в том числе в пакете Gensim. Основная задача алгоритмов ТМ, заключается в том что бы полученные темы были хорошего качество, понятными, самозначимыми и разделенными. Достижение этих целей во многом зависит от качества предварительной обработки текста и стратегии поиска оптимального количества тем. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pP8tUkU-Jzua"
      },
      "source": [
        "##Импорт пакетов\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rax0LrYWJqbw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf4a1a48-1f26-419c-a563-27a3c4e704cf"
      },
      "source": [
        "import nltk; nltk.download('stopwords')\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "# Plotting tools\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "# Enable logging for gensim - optional\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gpz436soKMkB"
      },
      "source": [
        "Подход LDA к тематическому моделированию заключается в том, что каждый документ рассматривается как набор тем в определенной пропорции. И каждая тема как набор ключевых слов, опять же, в определенной пропорции.\n",
        "\n",
        "После того, как вы укажете алгоритму количество тем, все, что он сделает, — это отобразит распределение тем в документах и распределение ключевых слов по темам.\n",
        "\n",
        "Тема — это не что иное, как набор доминирующих ключевых слов. Просто взглянув на ключевые слова, вы сможете определить, о чем эта тема.\n",
        "\n",
        "Ниже приведены ключевые факторы для получения хороших разделительных тем:\n",
        "\n",
        "Качество обработки текста.\n",
        "Разнообразие тем, о которых говорится в тексте.\n",
        "Выбор алгоритма моделирование тем.\n",
        "Количество тем, указанных в алгоритме.\n",
        "Алгоритмы настройки параметров."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzcGR_TJKQ2A"
      },
      "source": [
        "# Подготовим стоп-слова\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqWlQzsmKXGD"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = russian_stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5_VjjTTPGnq"
      },
      "source": [
        "##Лемматизация\n",
        "Лемматизация — это не что иное, как преобразование слова в его корневое слово. Например: лемма слова «machines» — это «machine». Аналогично, «walking» -> «walk», «mice» -> «mouse» и так далее."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNV6UsXyLTxd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e35f80d-3fbf-47f5-f942-72c0152c6259"
      },
      "source": [
        "!pip install pymorphy2\n",
        "import pymorphy2\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "ltexts = \"\"\n",
        "print(texts)\n",
        "for word in texts.split():\n",
        "    if len(str(word)) > 2:\n",
        "        ltexts+=\" \" + morph.parse(word)[0].normal_form\n",
        "print(ltexts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.7/dist-packages (0.9.1)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (2.4.417127.4579844)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.7.2)\n",
            " это открывает большие перспективы postgresql действительно вполне стримить использовать наши оптимизации показано растет размер wal справа зависит размера добавляемого куска слева самой строки посчитали скорость мб оригинального постгреса слева оптимизацией слева ограничились случаем добавляем байтные кусочки иначе всё слишком медленно видим скорость очень быстро падает мб кб полоса маленькая справа видим производительность дописывания зависит размера данных любых размеров добавлений предсказать хотим занять полосу мб апдейтить килобайтными чанками деградации производительности вместо заключения серии статей рассказал возможности улучшения postgresql эффективного хранения больших значений примере популярного типа данных jsonb быстрого дописывания бинарных данных postgresql славится своей расширяемостью поэтому логично расширить toast хранение больших значений datatype aware предложили серию патчей реализующих api toast см pluggable toast основе которого разрабатывать toast оптимизированный определенного типа данных например описанные оптимизации jsonb реализовать виде расширения надеемся закоммитить это следующей версии pg видео моего выступления saint highload конференция highload foundation пройдет марта москве крокус экспо февраля планируйте свое участие расписание полный список тезисами сайте наш доклад коллегами pluggable toast or one toast fits all логичным продолжением рассказал серии статей билеты купить встречи конференции теги postgresqlpostgresjsonjsonbstreamingstream processinghigh performancehighload postgresql performancetoast хабы блог компании конференции олега бунина онтико высокая производительностьpostgresqlадминистрирование баз данныххранилища данных конференции олега бунина онтико профессиональные конференции it разработчиков ×подписаться facebookfacebooktwittertelegram карма рейтинг oleg bartunov zen пользователь ×подписаться комментарии vtikunov интересно кейс мб одном поле кажется логичным postgres коробки нормально работает таким размером можете оставлять комментарии свежим постам похожие публикации апреля свободный доступ трансляции терминуса главного зала highload foundation января борьба toast будущее jsonb postgresql декабря проклятье toast каким маслом ест jsonb лучшие публикации сутки сегодня вторая жизнь китайского чудо шнурка\n",
            " это открывать больший перспектива postgresql действительно вполне стримитя использовать наш оптимизация показать расти размер wal справа зависеть размер добавлять кусок слева сам строка посчитать скорость оригинальный постгрес слева оптимизация слева ограничиться случаем добавлять байтный кусочек иначе всё слишком медленно видеть скорость очень быстро падать полоса маленький справа видеть производительность дописывание зависеть размер данные любой размер добавление предсказать хотеть занять полоса апдейтить килобайтный чанк деградация производительность вместо заключение серия стать рассказать возможность улучшение postgresql эффективный хранение больший значение пример популярный тип данные jsonb быстрый дописывание бинарный данные postgresql славиться свой расширяемость поэтому логично расширить toast хранение больший значение datatype aware предложить серия патч реализовать api toast pluggable toast основа который разрабатывать toast оптимизировать определённый тип данные например описать оптимизация jsonb реализовать вид расширение надеяться закоммитить это следующий версия видео мой выступление saint highload конференция highload foundation пройти март москва крокус экспо февраль планировать свой участие расписание полный список тезис сайт наш доклад коллега pluggable toast one toast fits all логичный продолжение рассказать серия стать билет купить встреча конференция тег postgresqlpostgresjsonjsonbstreamingstream processinghigh performancehighload postgresql performancetoast хаба блог компания конференция олег бунин онтико высокий производительностьpostgresqlадминистрирование база данныххранилище данные конференция олег бунин онтико профессиональный конференция разработчик ×подписаться facebookfacebooktwittertelegram карма рейтинг oleg bartunov zen пользователь ×подписаться комментарий vtikunov интересно кейс один поле казаться логичный postgres коробка нормально работать такой размер мочь оставлять комментарий свежий пост похожий публикация апрель свободный доступ трансляция терминус главное зал highload foundation январь борьба toast будущее jsonb postgresql декабрь проклятие toast какой масло есть jsonb хороший публикация сутки сегодня второй жизнь китайский чудо шнурок\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBXQ2V6WPqaR"
      },
      "source": [
        "##Создадим словарь и корпус.\n",
        "Двумя основными входными данными для тематической модели LDA являются словарь (id2word) и корпус. Давайте создадим их."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ckt4JD1YrLj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b01e2e97-111e-46ca-c258-5c49e92c0376"
      },
      "source": [
        "from nltk.util import ngrams\n",
        "def make_bigrams(ttexts):\n",
        "    texts = ' '.join(ttexts)\n",
        "    token = nltk.word_tokenize(ttexts)\n",
        "    bigrams = list(ngrams(token, 2))\n",
        "    # print(bigrams)\n",
        "    return bigrams\n",
        "def make_trigrams(ttexts):\n",
        "    texts = ' '.join(ttexts)\n",
        "    token = nltk.word_tokenize(ttexts)\n",
        "    trigrams = list(ngrams(token, 2))\n",
        "    return trigrams\n",
        "\n",
        "btexts = make_bigrams(texts) + make_trigrams(texts)\n",
        "id2word = corpora.Dictionary(btexts)\n",
        "corpus = [id2word.doc2bow(text) for text in btexts]\n",
        "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('открывает', 1), ('это', 1)]]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMFn6b2iRCuk"
      },
      "source": [
        "Или вы можете увидеть удобочитаемую форму самого корпуса."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuyeZ3OTRFJ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0033fe7-7825-450d-95cb-7105a870ebdc"
      },
      "source": [
        "# Human readable format of corpus (term-frequency)\n",
        "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('открывает', 1), ('это', 1)]]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1juBYJaaRLfH"
      },
      "source": [
        "##Построим тематическую модель\n",
        "У нас есть все необходимое для обучения модели LDA. В дополнение к корпусу и словарю необходимо также указать количество тем."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfhLNkHJRN2b"
      },
      "source": [
        "# Build LDA model\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=3,                          # Количество тем (отдельная тема для диссертации)\n",
        "                                           random_state=100,\n",
        "                                           update_every=1,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha='auto',\n",
        "                                           per_word_topics=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWjtOlCbRW43"
      },
      "source": [
        "##Просмотр тем в модели LDA\n",
        "Вышеупомянутая модель LDA построена на 20 различных темах, где каждая тема представляет собой комбинацию ключевых слов, и каждое ключевое слово вносит определенный вес в тему.\n",
        "\n",
        "Вы можете увидеть ключевые слова для каждой темы и вес (важность) каждого ключевого слова, используя lda_model.print_topics()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99Xxhtw7RSxU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48f1098a-a6e0-4fb9-8b29-bdecad9d96b3"
      },
      "source": [
        "lda_model.print_topics()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.102*\"toast\" + 0.028*\"конференции\" + 0.028*\"foundation\" + 0.020*\"highload\" + 0.017*\"борьба\" + 0.017*\"января\" + 0.017*\"каким\" + 0.017*\"проклятье\" + 0.017*\"будущее\" + 0.015*\"×подписаться\"'),\n",
              " (1,\n",
              "  '0.114*\"postgresql\" + 0.060*\"данных\" + 0.036*\"декабря\" + 0.023*\"значений\" + 0.023*\"видим\" + 0.023*\"больших\" + 0.021*\"справа\" + 0.021*\"размера\" + 0.021*\"скорость\" + 0.021*\"зависит\"'),\n",
              " (2,\n",
              "  '0.071*\"jsonb\" + 0.038*\"публикации\" + 0.031*\"сутки\" + 0.031*\"ест\" + 0.031*\"китайского\" + 0.031*\"чудо\" + 0.031*\"сегодня\" + 0.031*\"лучшие\" + 0.031*\"вторая\" + 0.031*\"жизнь\"')]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6IickF1RhIN"
      },
      "source": [
        "##Визуализация темы и ключевых слов\n",
        "Теперь, когда модель LDA создана, следующим шагом является изучение созданных тем и связанных с ними ключевых слов. Нет лучшего инструмента, чем интерактивная диаграмма пакета pyLDAvis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3gAdj_AR78T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1587781-5119-4340-be57-4889796d6dbe"
      },
      "source": [
        "!pip install pyLDAvis==2.1.2\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim  # don't skip this"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyLDAvis==2.1.2 in /usr/local/lib/python3.7/dist-packages (2.1.2)\n",
            "Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2) (1.7.3)\n",
            "Requirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2) (0.37.1)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2) (2.8.3)\n",
            "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2) (1.2.3)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2) (1.21.6)\n",
            "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2) (2.11.3)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2) (3.6.4)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2) (1.17)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis==2.1.2) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.7.2->pyLDAvis==2.1.2) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.17.0->pyLDAvis==2.1.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.17.0->pyLDAvis==2.1.2) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.17.0->pyLDAvis==2.1.2) (1.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from numexpr->pyLDAvis==2.1.2) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->numexpr->pyLDAvis==2.1.2) (3.0.9)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->pyLDAvis==2.1.2) (8.13.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->pyLDAvis==2.1.2) (21.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->pyLDAvis==2.1.2) (1.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->pyLDAvis==2.1.2) (57.4.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->pyLDAvis==2.1.2) (1.4.1)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->pyLDAvis==2.1.2) (0.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mdNzUz9RkAn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 861
        },
        "outputId": "e3e21268-eca2-4762-ea0d-bb8c540cd532"
      },
      "source": [
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
        "vis"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
              "topic                                                \n",
              "0      0.172455 -0.134411       1        1  44.419254\n",
              "2     -0.219469 -0.063271       2        1  33.125148\n",
              "1      0.047014  0.197682       3        1  22.455597, topic_info=           Term       Freq      Total Category  logprob  loglift\n",
              "4    postgresql  27.000000  27.000000  Default  30.0000  30.0000\n",
              "83        toast  47.000000  47.000000  Default  29.0000  29.0000\n",
              "74        jsonb  25.000000  25.000000  Default  28.0000  28.0000\n",
              "47       данных  14.000000  14.000000  Default  27.0000  27.0000\n",
              "187  публикации  13.000000  13.000000  Default  26.0000  26.0000\n",
              "..          ...        ...        ...      ...      ...      ...\n",
              "198     декабря   8.341245  14.096230   Topic3  -3.3360   0.9689\n",
              "20        слева   3.402987   5.937651   Topic3  -4.2325   0.9370\n",
              "37     медленно   1.586101   2.786178   Topic3  -4.9959   0.9302\n",
              "73         типа   2.965351   5.990049   Topic3  -4.3702   0.7905\n",
              "25           мб   3.862203   9.121221   Topic3  -4.1059   0.6343\n",
              "\n",
              "[145 rows x 6 columns], token_table=      Topic      Freq          Term\n",
              "term                               \n",
              "136       1  0.984332           all\n",
              "166       2  0.911629      bartunov\n",
              "135       1  0.984332          fits\n",
              "115       1  0.969786    foundation\n",
              "113       1  0.908879      highload\n",
              "...     ...       ...           ...\n",
              "120       1  0.984332         экспо\n",
              "1         2  0.900314           это\n",
              "67        3  0.889732  эффективного\n",
              "195       1  0.585245        января\n",
              "195       2  0.365778        января\n",
              "\n",
              "[118 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 3, 2])"
            ],
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el591398252823792165521426698\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el591398252823792165521426698_data = {\"mdsDat\": {\"x\": [0.1724552890998562, -0.21946939938993842, 0.047014110290082546], \"y\": [-0.13441102991532794, -0.06327099961217061, 0.19768202952749853], \"topics\": [1, 2, 3], \"cluster\": [1, 1, 1], \"Freq\": [44.419254303927744, 33.125148430156585, 22.45559726591567]}, \"tinfo\": {\"Term\": [\"postgresql\", \"toast\", \"jsonb\", \"\\u0434\\u0430\\u043d\\u043d\\u044b\\u0445\", \"\\u043f\\u0443\\u0431\\u043b\\u0438\\u043a\\u0430\\u0446\\u0438\\u0438\", \"\\u0434\\u0435\\u043a\\u0430\\u0431\\u0440\\u044f\", \"\\u0441\\u0435\\u0433\\u043e\\u0434\\u043d\\u044f\", \"\\u0436\\u0438\\u0437\\u043d\\u044c\", \"\\u0435\\u0441\\u0442\", \"\\u043b\\u0443\\u0447\\u0448\\u0438\\u0435\", \"\\u0432\\u0442\\u043e\\u0440\\u0430\\u044f\", \"\\u0441\\u0443\\u0442\\u043a\\u0438\", \"\\u043a\\u0438\\u0442\\u0430\\u0439\\u0441\\u043a\\u043e\\u0433\\u043e\", \"\\u0447\\u0443\\u0434\\u043e\", \"\\u043c\\u0430\\u0441\\u043b\\u043e\\u043c\", \"\\u043a\\u043e\\u043d\\u0444\\u0435\\u0440\\u0435\\u043d\\u0446\\u0438\\u0438\", \"foundation\", \"\\u0437\\u043d\\u0430\\u0447\\u0435\\u043d\\u0438\\u0439\", \"\\u0432\\u0438\\u0434\\u0438\\u043c\", \"\\u0431\\u043e\\u043b\\u044c\\u0448\\u0438\\u0445\", \"\\u0441\\u043f\\u0440\\u0430\\u0432\\u0430\", \"\\u0440\\u0430\\u0437\\u043c\\u0435\\u0440\\u0430\", \"\\u0441\\u043a\\u043e\\u0440\\u043e\\u0441\\u0442\\u044c\", \"\\u0437\\u0430\\u0432\\u0438\\u0441\\u0438\\u0442\", \"highload\", \"\\u043c\\u0431\", \"\\u0448\\u043d\\u0443\\u0440\\u043a\\u0430\", \"\\u043a\\u043e\\u043c\\u043c\\u0435\\u043d\\u0442\\u0430\\u0440\\u0438\\u0438\", \"\\u0441\\u043b\\u0435\\u0432\\u0430\", \"\\u043f\\u0440\\u043e\\u0438\\u0437\\u0432\\u043e\\u0434\\u0438\\u0442\\u0435\\u043b\\u044c\\u043d\\u043e\\u0441\\u0442\\u0438\", \"toast\", \"foundation\", \"\\u043a\\u043e\\u043d\\u0444\\u0435\\u0440\\u0435\\u043d\\u0446\\u0438\\u0438\", \"highload\", \"\\u043b\\u043e\\u0433\\u0438\\u0447\\u043d\\u044b\\u043c\", \"\\u00d7\\u043f\\u043e\\u0434\\u043f\\u0438\\u0441\\u0430\\u0442\\u044c\\u0441\\u044f\", \"\\u0431\\u0443\\u043d\\u0438\\u043d\\u0430\", \"\\u043e\\u043d\\u0442\\u0438\\u043a\\u043e\", \"\\u043e\\u043b\\u0435\\u0433\\u0430\", \"pluggable\", \"\\u0441\\u0432\\u043e\\u0435\", \"\\u043d\\u0430\\u0448\", \"postgresqlpostgresjsonjsonbstreamingstream\", \"\\u043a\\u0443\\u043f\\u0438\\u0442\\u044c\", \"all\", \"fits\", \"processinghigh\", \"\\u0441\\u043f\\u0438\\u0441\\u043e\\u043a\", \"\\u0441\\u0430\\u0439\\u0442\\u0435\", \"\\u0442\\u0435\\u0437\\u0438\\u0441\\u0430\\u043c\\u0438\", \"\\u0431\\u043b\\u043e\\u0433\", \"\\u043f\\u043e\\u043b\\u043d\\u044b\\u0439\", \"\\u0440\\u0430\\u0441\\u043f\\u0438\\u0441\\u0430\\u043d\\u0438\\u0435\", \"\\u0443\\u0447\\u0430\\u0441\\u0442\\u0438\\u0435\", \"\\u043f\\u043b\\u0430\\u043d\\u0438\\u0440\\u0443\\u0439\\u0442\\u0435\", \"\\u0444\\u0435\\u0432\\u0440\\u0430\\u043b\\u044f\", \"\\u044d\\u043a\\u0441\\u043f\\u043e\", \"\\u043a\\u0440\\u043e\\u043a\\u0443\\u0441\", \"\\u043c\\u043e\\u0441\\u043a\\u0432\\u0435\", \"\\u043c\\u0430\\u0440\\u0442\\u0430\", \"\\u0431\\u043e\\u0440\\u044c\\u0431\\u0430\", \"\\u044f\\u043d\\u0432\\u0430\\u0440\\u044f\", \"\\u043f\\u0440\\u043e\\u043a\\u043b\\u044f\\u0442\\u044c\\u0435\", \"\\u043a\\u0430\\u043a\\u0438\\u043c\", \"\\u0431\\u0443\\u0434\\u0443\\u0449\\u0435\\u0435\", \"\\u0445\\u0430\\u0431\\u044b\", \"\\u0434\\u043e\\u043a\\u043b\\u0430\\u0434\", \"\\u0431\\u0430\\u0437\", \"one\", \"or\", \"\\u043f\\u0440\\u043e\\u0434\\u043e\\u043b\\u0436\\u0435\\u043d\\u0438\\u0435\\u043c\", \"\\u043a\\u043e\\u043b\\u043b\\u0435\\u0433\\u0430\\u043c\\u0438\", \"\\u0432\\u0441\\u0442\\u0440\\u0435\\u0447\\u0438\", \"\\u0442\\u0435\\u0433\\u0438\", \"jsonb\", \"\\u043f\\u0443\\u0431\\u043b\\u0438\\u043a\\u0430\\u0446\\u0438\\u0438\", \"\\u0447\\u0443\\u0434\\u043e\", \"\\u043a\\u0438\\u0442\\u0430\\u0439\\u0441\\u043a\\u043e\\u0433\\u043e\", \"\\u0436\\u0438\\u0437\\u043d\\u044c\", \"\\u0432\\u0442\\u043e\\u0440\\u0430\\u044f\", \"\\u0441\\u0435\\u0433\\u043e\\u0434\\u043d\\u044f\", \"\\u0441\\u0443\\u0442\\u043a\\u0438\", \"\\u043b\\u0443\\u0447\\u0448\\u0438\\u0435\", \"\\u0435\\u0441\\u0442\", \"\\u043c\\u0430\\u0441\\u043b\\u043e\\u043c\", \"\\u0448\\u043d\\u0443\\u0440\\u043a\\u0430\", \"\\u043a\\u043e\\u043c\\u043c\\u0435\\u043d\\u0442\\u0430\\u0440\\u0438\\u0438\", \"\\u0441\\u0442\\u0430\\u0442\\u0435\\u0439\", \"\\u0441\\u0435\\u0440\\u0438\\u0438\", \"\\u043e\\u043f\\u0442\\u0438\\u043c\\u0438\\u0437\\u0430\\u0446\\u0438\\u0438\", \"\\u0434\\u043e\\u043f\\u0438\\u0441\\u044b\\u0432\\u0430\\u043d\\u0438\\u044f\", \"\\u044d\\u0442\\u043e\", \"\\u0440\\u0435\\u0439\\u0442\\u0438\\u043d\\u0433\", \"oleg\", \"bartunov\", \"\\u043f\\u043e\\u043b\\u0435\", \"\\u043a\\u0430\\u0440\\u043c\\u0430\", \"zen\", \"vtikunov\", \"\\u0438\\u043d\\u0442\\u0435\\u0440\\u0435\\u0441\\u043d\\u043e\", \"\\u043a\\u0435\\u0439\\u0441\", \"\\u043e\\u0434\\u043d\\u043e\\u043c\", \"\\u0440\\u0430\\u0437\\u043c\\u0435\\u0440\\u043e\\u043c\", \"\\u043a\\u043e\\u0440\\u043e\\u0431\\u043a\\u0438\", \"\\u0440\\u0430\\u0431\\u043e\\u0442\\u0430\\u0435\\u0442\", \"\\u043d\\u043e\\u0440\\u043c\\u0430\\u043b\\u044c\\u043d\\u043e\", \"\\u043c\\u0431\", \"\\u0431\\u0443\\u0434\\u0443\\u0449\\u0435\\u0435\", \"\\u043a\\u0430\\u043a\\u0438\\u043c\", \"\\u043f\\u0440\\u043e\\u043a\\u043b\\u044f\\u0442\\u044c\\u0435\", \"\\u044f\\u043d\\u0432\\u0430\\u0440\\u044f\", \"\\u0431\\u043e\\u0440\\u044c\\u0431\\u0430\", \"\\u0434\\u0435\\u043a\\u0430\\u0431\\u0440\\u044f\", \"postgresql\", \"\\u0434\\u0430\\u043d\\u043d\\u044b\\u0445\", \"\\u0437\\u043d\\u0430\\u0447\\u0435\\u043d\\u0438\\u0439\", \"\\u0432\\u0438\\u0434\\u0438\\u043c\", \"\\u0431\\u043e\\u043b\\u044c\\u0448\\u0438\\u0445\", \"\\u0441\\u043f\\u0440\\u0430\\u0432\\u0430\", \"\\u0440\\u0430\\u0437\\u043c\\u0435\\u0440\\u0430\", \"\\u0441\\u043a\\u043e\\u0440\\u043e\\u0441\\u0442\\u044c\", \"\\u0437\\u0430\\u0432\\u0438\\u0441\\u0438\\u0442\", \"\\u043b\\u044e\\u0431\\u044b\\u0445\", \"\\u0445\\u0440\\u0430\\u043d\\u0435\\u043d\\u0438\\u044f\", \"\\u0434\\u0435\\u0433\\u0440\\u0430\\u0434\\u0430\\u0446\\u0438\\u0438\", \"\\u0432\\u043c\\u0435\\u0441\\u0442\\u043e\", \"\\u043f\\u0440\\u043e\\u0438\\u0437\\u0432\\u043e\\u0434\\u0438\\u0442\\u0435\\u043b\\u044c\\u043d\\u043e\\u0441\\u0442\\u0438\", \"\\u0445\\u043e\\u0442\\u0438\\u043c\", \"\\u044d\\u0444\\u0444\\u0435\\u043a\\u0442\\u0438\\u0432\\u043d\\u043e\\u0433\\u043e\", \"\\u043c\\u0430\\u043b\\u0435\\u043d\\u044c\\u043a\\u0430\\u044f\", \"\\u043a\\u0431\", \"\\u043f\\u043e\\u043b\\u043e\\u0441\\u0430\", \"\\u0437\\u0430\\u043a\\u043b\\u044e\\u0447\\u0435\\u043d\\u0438\\u044f\", \"\\u043e\\u0433\\u0440\\u0430\\u043d\\u0438\\u0447\\u0438\\u043b\\u0438\\u0441\\u044c\", \"\\u0441\\u0430\\u043c\\u043e\\u0439\", \"\\u0441\\u0442\\u0440\\u043e\\u043a\\u0438\", \"\\u043f\\u043e\\u0441\\u0447\\u0438\\u0442\\u0430\\u043b\\u0438\", \"\\u043f\\u0435\\u0440\\u0441\\u043f\\u0435\\u043a\\u0442\\u0438\\u0432\\u044b\", \"\\u0434\\u043e\\u0431\\u0430\\u0432\\u043b\\u044f\\u0435\\u043c\\u043e\\u0433\\u043e\", \"\\u043a\\u0443\\u0441\\u043a\\u0430\", \"\\u0434\\u0435\\u043a\\u0430\\u0431\\u0440\\u044f\", \"\\u0441\\u043b\\u0435\\u0432\\u0430\", \"\\u043c\\u0435\\u0434\\u043b\\u0435\\u043d\\u043d\\u043e\", \"\\u0442\\u0438\\u043f\\u0430\", \"\\u043c\\u0431\"], \"Freq\": [27.0, 47.0, 25.0, 14.0, 13.0, 14.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 13.0, 13.0, 6.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 9.0, 9.0, 6.0, 5.0, 5.0, 3.0, 47.24055433073426, 12.85448962560272, 13.095318564932201, 9.351776871838078, 7.033343228024675, 7.174790679668737, 6.710813894423565, 6.710813894423565, 6.710813894423565, 6.124653623205906, 3.5131396278393003, 3.5131391959505534, 3.5131391959505534, 3.5131391959505534, 3.5131391959505534, 3.5131391959505534, 3.5131391959505534, 3.5131391959505534, 3.5131391959505534, 3.5131391959505534, 3.5131391959505534, 3.5131391959505534, 3.5131391959505534, 3.5131391959505534, 3.5131391959505534, 3.5131391959505534, 3.5131391959505534, 3.5131391959505534, 3.5131391959505534, 3.5131391959505534, 7.896897435559808, 7.896897435559808, 7.896894844227326, 7.896894844227326, 7.896894844227326, 3.5131391959505534, 3.5131391959505534, 3.5131391959505534, 3.51313898000618, 3.51313898000618, 3.51313898000618, 3.51313898000618, 3.51313898000618, 3.51313898000618, 24.51225117464377, 13.08648665084751, 10.661667390872775, 10.661667390872775, 10.661667390872775, 10.661667390872775, 10.661667390872775, 10.661667390872775, 10.661667390872775, 10.661667390872775, 10.661666102568496, 5.439482192673015, 5.057721884293749, 4.054029142279974, 4.039844912165664, 3.3507654153456055, 3.466867719065782, 2.683476458142459, 2.6421191531326187, 2.6421191531326187, 2.6421191531326187, 2.642118831056549, 2.642118831056549, 2.642118831056549, 2.642118831056549, 2.642118831056549, 2.642118831056549, 2.642118831056549, 2.642118831056549, 2.642118831056549, 2.642118831056549, 2.642118831056549, 4.943550426612068, 5.439361092070768, 5.439360447918628, 5.439360447918628, 5.4393598037664885, 5.4393598037664885, 5.439511823671437, 26.773106127325494, 13.981946446215254, 5.471207029515193, 5.3448437926094226, 5.344843355937537, 4.88557020690223, 4.88557020690223, 4.885569333558459, 4.8807148522064505, 2.8390340542482893, 2.8390340542482893, 2.8390340542482893, 2.8390340542482893, 2.8390340542482893, 2.8390340542482893, 2.8390340542482893, 2.8390340542482893, 2.8390340542482893, 2.8390340542482893, 2.8243952842920015, 2.3797604685410967, 2.3797604685410967, 2.3797604685410967, 2.3797604685410967, 2.379760031869211, 2.3782229468319644, 2.3778609458388176, 8.341245224311646, 3.4029866243670512, 1.5861014564029878, 2.9653512222701304, 3.8622030060572325], \"Total\": [27.0, 47.0, 25.0, 14.0, 13.0, 14.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 13.0, 13.0, 6.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 9.0, 9.0, 6.0, 5.0, 5.0, 3.0, 47.79325051528295, 13.405018172268337, 13.657301018247974, 9.902305418503694, 7.583871774690293, 7.738969695881976, 7.261342561867709, 7.261342561867709, 7.261342561867709, 6.6751822705202954, 4.0636683154131985, 4.0636678835244515, 4.0636678835244515, 4.0636678835244515, 4.0636678835244515, 4.0636678835244515, 4.0636678835244515, 4.0636678835244515, 4.0636678835244515, 4.0636678835244515, 4.0636678835244515, 4.0636678835244515, 4.0636678835244515, 4.0636678835244515, 4.0636678835244515, 4.0636678835244515, 4.0636678835244515, 4.0636678835244515, 4.0636678835244515, 4.0636678835244515, 13.66948335648498, 13.66948335648498, 13.669481409304636, 13.669481409304636, 13.669482053456777, 4.0636678835244515, 4.0636678835244515, 4.0636678835244515, 4.063667667580078, 4.063667667580078, 4.063667667580078, 4.063667667580078, 4.063667667580078, 4.063667667580078, 25.201961624725072, 13.735181233461597, 11.310363647055757, 11.310363647055757, 11.310363647055757, 11.310363647055757, 11.310363647055757, 11.310363647055757, 11.310363647055757, 11.310363647055757, 11.310362358751478, 6.0881785838212314, 5.719794220846431, 4.7027221323043085, 4.710289396474601, 3.999458432362986, 4.144064399580663, 3.332169421173746, 3.2908131688927265, 3.2908131688927265, 3.2908131688927265, 3.2908128468166566, 3.2908128468166566, 3.2908128468166566, 3.2908128468166566, 3.2908128468166566, 3.2908128468166566, 3.2908128468166566, 3.2908128468166566, 3.2908128468166566, 3.2908128468166566, 3.2908128468166566, 9.121221385256819, 13.669482053456777, 13.669481409304636, 13.669481409304636, 13.66948335648498, 13.66948335648498, 14.096230237221658, 27.313772288432723, 14.5187292587116, 6.003977452789399, 5.877612690546429, 5.877612253874544, 5.418339004190464, 5.418339004190464, 5.418339129589421, 5.416649880068556, 3.371802999308097, 3.371802999308097, 3.371802999308097, 3.371802999308097, 3.371802999308097, 3.371802999308097, 3.371802999308097, 3.371802999308097, 3.371802999308097, 3.371802999308097, 3.366710574605168, 2.9125295019034594, 2.9125295019034594, 2.9125295019034594, 2.9125295019034594, 2.912529085361328, 2.912447532654056, 2.9124282756411692, 14.096230237221658, 5.937650583468783, 2.7861782317488455, 5.9900488009096, 9.121221385256819], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -2.28410005569458, -3.585599899291992, -3.5671000480651855, -3.9038000106811523, -4.188700199127197, -4.168700218200684, -4.235599994659424, -4.235599994659424, -4.235599994659424, -4.327000141143799, -4.882800102233887, -4.882800102233887, -4.882800102233887, -4.882800102233887, -4.882800102233887, -4.882800102233887, -4.882800102233887, -4.882800102233887, -4.882800102233887, -4.882800102233887, -4.882800102233887, -4.882800102233887, -4.882800102233887, -4.882800102233887, -4.882800102233887, -4.882800102233887, -4.882800102233887, -4.882800102233887, -4.882800102233887, -4.882800102233887, -4.072800159454346, -4.072800159454346, -4.072800159454346, -4.072800159454346, -4.072800159454346, -4.882800102233887, -4.882800102233887, -4.882800102233887, -4.882800102233887, -4.882800102233887, -4.882800102233887, -4.882800102233887, -4.882800102233887, -4.882800102233887, -2.6468000411987305, -3.274399995803833, -3.479300022125244, -3.479300022125244, -3.479300022125244, -3.479300022125244, -3.479300022125244, -3.479300022125244, -3.479300022125244, -3.479300022125244, -3.479300022125244, -4.152299880981445, -4.224999904632568, -4.446199893951416, -4.449699878692627, -4.63670015335083, -4.602700233459473, -4.858799934387207, -4.8744001388549805, -4.8744001388549805, -4.8744001388549805, -4.8744001388549805, -4.8744001388549805, -4.8744001388549805, -4.8744001388549805, -4.8744001388549805, -4.8744001388549805, -4.8744001388549805, -4.8744001388549805, -4.8744001388549805, -4.8744001388549805, -4.8744001388549805, -4.247900009155273, -4.152299880981445, -4.152299880981445, -4.152299880981445, -4.152299880981445, -4.152299880981445, -4.152200222015381, -2.169800043106079, -2.8194000720977783, -3.757699966430664, -3.781100034713745, -3.781100034713745, -3.8708999156951904, -3.8708999156951904, -3.8708999156951904, -3.8719000816345215, -4.413700103759766, -4.413700103759766, -4.413700103759766, -4.413700103759766, -4.413700103759766, -4.413700103759766, -4.413700103759766, -4.413700103759766, -4.413700103759766, -4.413700103759766, -4.418900012969971, -4.590199947357178, -4.590199947357178, -4.590199947357178, -4.590199947357178, -4.590199947357178, -4.590799808502197, -4.591000080108643, -3.3359999656677246, -4.232500076293945, -4.9959001541137695, -4.370200157165527, -4.105899810791016], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.7999, 0.7696, 0.7695, 0.7543, 0.7361, 0.7358, 0.7327, 0.7327, 0.7327, 0.7254, 0.6659, 0.6659, 0.6659, 0.6659, 0.6659, 0.6659, 0.6659, 0.6659, 0.6659, 0.6659, 0.6659, 0.6659, 0.6659, 0.6659, 0.6659, 0.6659, 0.6659, 0.6659, 0.6659, 0.6659, 0.2628, 0.2628, 0.2628, 0.2628, 0.2628, 0.6659, 0.6659, 0.6659, 0.6659, 0.6659, 0.6659, 0.6659, 0.6659, 0.6659, 1.0771, 1.0565, 1.0458, 1.0458, 1.0458, 1.0458, 1.0458, 1.0458, 1.0458, 1.0458, 1.0458, 0.9922, 0.9819, 0.9564, 0.9513, 0.9279, 0.9265, 0.8884, 0.8853, 0.8853, 0.8853, 0.8853, 0.8853, 0.8853, 0.8853, 0.8853, 0.8853, 0.8853, 0.8853, 0.8853, 0.8853, 0.8853, 0.4924, 0.1834, 0.1834, 0.1834, 0.1834, 0.1834, 0.1527, 1.4736, 1.456, 1.4007, 1.3986, 1.3986, 1.3901, 1.3901, 1.3901, 1.3894, 1.3216, 1.3216, 1.3216, 1.3216, 1.3216, 1.3216, 1.3216, 1.3216, 1.3216, 1.3216, 1.318, 1.2916, 1.2916, 1.2916, 1.2916, 1.2916, 1.291, 1.2908, 0.9689, 0.937, 0.9302, 0.7905, 0.6343]}, \"token.table\": {\"Topic\": [1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 3, 1, 1, 1, 2, 2, 1, 1, 1, 3, 1, 2, 1, 2, 1, 3, 3, 1, 2, 3, 3, 2, 3, 3, 1, 2, 2, 2, 3, 3, 3, 2, 1, 2, 2, 3, 2, 2, 1, 2, 1, 2, 1, 1, 3, 1, 2, 3, 3, 1, 2, 2, 3, 2, 3, 1, 1, 2, 3, 2, 1, 1, 2, 3, 1, 2, 1, 3, 3, 1, 3, 1, 2, 2, 2, 3, 2, 1, 2, 1, 3, 1, 2, 2, 3, 2, 3, 1, 3, 2, 3, 2, 1, 1, 1, 3, 1, 1, 1, 3, 3, 2, 2, 1, 2, 3, 1, 2], \"Freq\": [0.9843324096975091, 0.9116287817121573, 0.9843324096975091, 0.969786078089307, 0.9088792578728562, 0.9919862736189975, 0.9116287817121573, 0.9843324620051934, 0.9843324620051934, 0.8988518600455132, 0.9885123048870988, 0.9843324096975091, 0.9843324096975091, 0.9834024573191713, 0.9116288709344342, 0.9116288709344342, 0.9045131684292299, 0.9843324096975091, 0.9843324096975091, 0.8506855818370771, 0.585245235051601, 0.36577827190725065, 0.5852452908394534, 0.3657783067746584, 0.9640090576031868, 0.850685518636166, 0.88973169565826, 0.9843324620051934, 0.9725593573521795, 0.9642717176229215, 0.88973169565826, 0.3547047626107369, 0.5675276201771791, 0.6867076496919549, 0.9843324096975091, 0.7239269737949945, 0.9725593573521795, 0.9725593573521795, 0.923079783760496, 0.8910774875122213, 0.8327812752989339, 0.9116288709344342, 0.5852453184181885, 0.36577832401136784, 0.9116288709344342, 0.88973169565826, 0.9116288709344342, 0.9725593573521795, 0.9843324620051934, 0.8741573222646611, 0.9518718217186739, 0.9116288709344342, 0.9843324096975091, 0.9843324096975091, 0.6867121902116889, 0.9230113862632999, 0.9725593573521795, 0.88973169565826, 0.88973169565826, 0.9843324096975091, 0.9725594681313342, 0.5481722007188425, 0.43853776057507404, 0.35891458364180595, 0.7178291672836119, 0.9843324096975091, 0.9843324096975091, 0.9116288709344342, 0.6866883232231353, 0.9116288709344342, 0.9640090576031868, 0.9640090576031868, 0.7501015576820286, 0.6866884214314654, 0.9843324096975091, 0.9116288709344342, 0.9843324096975091, 0.88973169565826, 0.6866883232231353, 0.9843324620051934, 0.88973169565826, 0.5852453184181885, 0.36577832401136784, 0.9464745880694642, 0.9116288709344342, 0.9227920209741534, 0.9116288709344342, 0.9843324096975091, 0.9116287817121573, 0.9843324096975091, 0.6866883232231353, 0.9843323050821571, 0.9725593573521795, 0.8492047225365358, 0.92279199961758, 0.33683356268357534, 0.505250344025363, 0.9843324096975091, 0.9227920209741534, 0.8505711984390669, 0.6866883232231353, 0.9725593573521795, 0.9843324620051934, 0.9843324096975091, 0.500830644241904, 0.500830644241904, 0.9843324096975091, 0.9843324096975091, 0.9843324096975091, 0.88973169565826, 0.88973169565826, 0.9725593573521795, 0.8212636885007012, 0.9843324096975091, 0.9003143660514296, 0.88973169565826, 0.585245235051601, 0.36577827190725065], \"Term\": [\"all\", \"bartunov\", \"fits\", \"foundation\", \"highload\", \"jsonb\", \"oleg\", \"one\", \"or\", \"pluggable\", \"postgresql\", \"postgresqlpostgresjsonjsonbstreamingstream\", \"processinghigh\", \"toast\", \"vtikunov\", \"zen\", \"\\u00d7\\u043f\\u043e\\u0434\\u043f\\u0438\\u0441\\u0430\\u0442\\u044c\\u0441\\u044f\", \"\\u0431\\u0430\\u0437\", \"\\u0431\\u043b\\u043e\\u0433\", \"\\u0431\\u043e\\u043b\\u044c\\u0448\\u0438\\u0445\", \"\\u0431\\u043e\\u0440\\u044c\\u0431\\u0430\", \"\\u0431\\u043e\\u0440\\u044c\\u0431\\u0430\", \"\\u0431\\u0443\\u0434\\u0443\\u0449\\u0435\\u0435\", \"\\u0431\\u0443\\u0434\\u0443\\u0449\\u0435\\u0435\", \"\\u0431\\u0443\\u043d\\u0438\\u043d\\u0430\", \"\\u0432\\u0438\\u0434\\u0438\\u043c\", \"\\u0432\\u043c\\u0435\\u0441\\u0442\\u043e\", \"\\u0432\\u0441\\u0442\\u0440\\u0435\\u0447\\u0438\", \"\\u0432\\u0442\\u043e\\u0440\\u0430\\u044f\", \"\\u0434\\u0430\\u043d\\u043d\\u044b\\u0445\", \"\\u0434\\u0435\\u0433\\u0440\\u0430\\u0434\\u0430\\u0446\\u0438\\u0438\", \"\\u0434\\u0435\\u043a\\u0430\\u0431\\u0440\\u044f\", \"\\u0434\\u0435\\u043a\\u0430\\u0431\\u0440\\u044f\", \"\\u0434\\u043e\\u0431\\u0430\\u0432\\u043b\\u044f\\u0435\\u043c\\u043e\\u0433\\u043e\", \"\\u0434\\u043e\\u043a\\u043b\\u0430\\u0434\", \"\\u0434\\u043e\\u043f\\u0438\\u0441\\u044b\\u0432\\u0430\\u043d\\u0438\\u044f\", \"\\u0435\\u0441\\u0442\", \"\\u0436\\u0438\\u0437\\u043d\\u044c\", \"\\u0437\\u0430\\u0432\\u0438\\u0441\\u0438\\u0442\", \"\\u0437\\u0430\\u043a\\u043b\\u044e\\u0447\\u0435\\u043d\\u0438\\u044f\", \"\\u0437\\u043d\\u0430\\u0447\\u0435\\u043d\\u0438\\u0439\", \"\\u0438\\u043d\\u0442\\u0435\\u0440\\u0435\\u0441\\u043d\\u043e\", \"\\u043a\\u0430\\u043a\\u0438\\u043c\", \"\\u043a\\u0430\\u043a\\u0438\\u043c\", \"\\u043a\\u0430\\u0440\\u043c\\u0430\", \"\\u043a\\u0431\", \"\\u043a\\u0435\\u0439\\u0441\", \"\\u043a\\u0438\\u0442\\u0430\\u0439\\u0441\\u043a\\u043e\\u0433\\u043e\", \"\\u043a\\u043e\\u043b\\u043b\\u0435\\u0433\\u0430\\u043c\\u0438\", \"\\u043a\\u043e\\u043c\\u043c\\u0435\\u043d\\u0442\\u0430\\u0440\\u0438\\u0438\", \"\\u043a\\u043e\\u043d\\u0444\\u0435\\u0440\\u0435\\u043d\\u0446\\u0438\\u0438\", \"\\u043a\\u043e\\u0440\\u043e\\u0431\\u043a\\u0438\", \"\\u043a\\u0440\\u043e\\u043a\\u0443\\u0441\", \"\\u043a\\u0443\\u043f\\u0438\\u0442\\u044c\", \"\\u043a\\u0443\\u0441\\u043a\\u0430\", \"\\u043b\\u043e\\u0433\\u0438\\u0447\\u043d\\u044b\\u043c\", \"\\u043b\\u0443\\u0447\\u0448\\u0438\\u0435\", \"\\u043b\\u044e\\u0431\\u044b\\u0445\", \"\\u043c\\u0430\\u043b\\u0435\\u043d\\u044c\\u043a\\u0430\\u044f\", \"\\u043c\\u0430\\u0440\\u0442\\u0430\", \"\\u043c\\u0430\\u0441\\u043b\\u043e\\u043c\", \"\\u043c\\u0431\", \"\\u043c\\u0431\", \"\\u043c\\u0435\\u0434\\u043b\\u0435\\u043d\\u043d\\u043e\", \"\\u043c\\u0435\\u0434\\u043b\\u0435\\u043d\\u043d\\u043e\", \"\\u043c\\u043e\\u0441\\u043a\\u0432\\u0435\", \"\\u043d\\u0430\\u0448\", \"\\u043d\\u043e\\u0440\\u043c\\u0430\\u043b\\u044c\\u043d\\u043e\", \"\\u043e\\u0433\\u0440\\u0430\\u043d\\u0438\\u0447\\u0438\\u043b\\u0438\\u0441\\u044c\", \"\\u043e\\u0434\\u043d\\u043e\\u043c\", \"\\u043e\\u043b\\u0435\\u0433\\u0430\", \"\\u043e\\u043d\\u0442\\u0438\\u043a\\u043e\", \"\\u043e\\u043f\\u0442\\u0438\\u043c\\u0438\\u0437\\u0430\\u0446\\u0438\\u0438\", \"\\u043f\\u0435\\u0440\\u0441\\u043f\\u0435\\u043a\\u0442\\u0438\\u0432\\u044b\", \"\\u043f\\u043b\\u0430\\u043d\\u0438\\u0440\\u0443\\u0439\\u0442\\u0435\", \"\\u043f\\u043e\\u043b\\u0435\", \"\\u043f\\u043e\\u043b\\u043d\\u044b\\u0439\", \"\\u043f\\u043e\\u043b\\u043e\\u0441\\u0430\", \"\\u043f\\u043e\\u0441\\u0447\\u0438\\u0442\\u0430\\u043b\\u0438\", \"\\u043f\\u0440\\u043e\\u0434\\u043e\\u043b\\u0436\\u0435\\u043d\\u0438\\u0435\\u043c\", \"\\u043f\\u0440\\u043e\\u0438\\u0437\\u0432\\u043e\\u0434\\u0438\\u0442\\u0435\\u043b\\u044c\\u043d\\u043e\\u0441\\u0442\\u0438\", \"\\u043f\\u0440\\u043e\\u043a\\u043b\\u044f\\u0442\\u044c\\u0435\", \"\\u043f\\u0440\\u043e\\u043a\\u043b\\u044f\\u0442\\u044c\\u0435\", \"\\u043f\\u0443\\u0431\\u043b\\u0438\\u043a\\u0430\\u0446\\u0438\\u0438\", \"\\u0440\\u0430\\u0431\\u043e\\u0442\\u0430\\u0435\\u0442\", \"\\u0440\\u0430\\u0437\\u043c\\u0435\\u0440\\u0430\", \"\\u0440\\u0430\\u0437\\u043c\\u0435\\u0440\\u043e\\u043c\", \"\\u0440\\u0430\\u0441\\u043f\\u0438\\u0441\\u0430\\u043d\\u0438\\u0435\", \"\\u0440\\u0435\\u0439\\u0442\\u0438\\u043d\\u0433\", \"\\u0441\\u0430\\u0439\\u0442\\u0435\", \"\\u0441\\u0430\\u043c\\u043e\\u0439\", \"\\u0441\\u0432\\u043e\\u0435\", \"\\u0441\\u0435\\u0433\\u043e\\u0434\\u043d\\u044f\", \"\\u0441\\u0435\\u0440\\u0438\\u0438\", \"\\u0441\\u043a\\u043e\\u0440\\u043e\\u0441\\u0442\\u044c\", \"\\u0441\\u043b\\u0435\\u0432\\u0430\", \"\\u0441\\u043b\\u0435\\u0432\\u0430\", \"\\u0441\\u043f\\u0438\\u0441\\u043e\\u043a\", \"\\u0441\\u043f\\u0440\\u0430\\u0432\\u0430\", \"\\u0441\\u0442\\u0430\\u0442\\u0435\\u0439\", \"\\u0441\\u0442\\u0440\\u043e\\u043a\\u0438\", \"\\u0441\\u0443\\u0442\\u043a\\u0438\", \"\\u0442\\u0435\\u0433\\u0438\", \"\\u0442\\u0435\\u0437\\u0438\\u0441\\u0430\\u043c\\u0438\", \"\\u0442\\u0438\\u043f\\u0430\", \"\\u0442\\u0438\\u043f\\u0430\", \"\\u0443\\u0447\\u0430\\u0441\\u0442\\u0438\\u0435\", \"\\u0444\\u0435\\u0432\\u0440\\u0430\\u043b\\u044f\", \"\\u0445\\u0430\\u0431\\u044b\", \"\\u0445\\u043e\\u0442\\u0438\\u043c\", \"\\u0445\\u0440\\u0430\\u043d\\u0435\\u043d\\u0438\\u044f\", \"\\u0447\\u0443\\u0434\\u043e\", \"\\u0448\\u043d\\u0443\\u0440\\u043a\\u0430\", \"\\u044d\\u043a\\u0441\\u043f\\u043e\", \"\\u044d\\u0442\\u043e\", \"\\u044d\\u0444\\u0444\\u0435\\u043a\\u0442\\u0438\\u0432\\u043d\\u043e\\u0433\\u043e\", \"\\u044f\\u043d\\u0432\\u0430\\u0440\\u044f\", \"\\u044f\\u043d\\u0432\\u0430\\u0440\\u044f\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 3, 2]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el591398252823792165521426698\", ldavis_el591398252823792165521426698_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el591398252823792165521426698\", ldavis_el591398252823792165521426698_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el591398252823792165521426698\", ldavis_el591398252823792165521426698_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    }
  ]
}